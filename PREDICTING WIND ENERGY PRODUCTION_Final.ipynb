{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07e4b7a8",
   "metadata": {},
   "source": [
    "## PREDICTING WIND ENERGY PRODUCTION\n",
    "\n",
    "The objetive of this project is to develop a machine learing model for estimating how much energy is going to be produced at the Sotavento experimental wind farm. To do it, we will use meteorological variables forecasted by ECMWF (http://www.ecmwf.int/) as input attributes. We have 22 variables, but, it is common practice to use the value of those variables, not just at the location of interest (Sotavento in this case), but at points in a grid around Sotavento. A 5x5 grid will be used in this case. Important to have in mind that the values at point 13 are the ones exactly Sotavento.Therefore, finally we have 550 variables (plus other 5 measuring energy, year, month, day and hour). The 22 variables are described as follows: \n",
    "\n",
    "- t2m: 2 metre temperature\n",
    "- u10: 10 metre U wind component\n",
    "- v10: 10 metre V wind component\n",
    "- u100: 100 metre U wind component\n",
    "- v100: 100 metre V wind component\n",
    "- cape: Convective available potential energy\n",
    "- flsr: Forecast logarithm of surface roughness for heat\n",
    "- fsr: Forecast surface roughness\n",
    "- iews: Instantaneous eastward turbulent surface stress\n",
    "- inss: Instantaneous northward turbulent surface\n",
    "- lai_hv: Leaf area index, high vegetation\n",
    "- lai_lv: Leaf area index, low vegetation\n",
    "- u10n: Neutral wind at 10 m u-component\n",
    "- v10n: Neutral wind at 10 m v-component\n",
    "- stl1: Soil temperature level 1\n",
    "- stl2: Soil temperature level 2\n",
    "- stl3: Soil temperature level 3\n",
    "- stl4: Soil temperature level 4\n",
    "- sp: Surface pressure\n",
    "- p54.162: Vertical integral of temperature\n",
    "- p59.162: Vertical integral of divergence of kinetic energy\n",
    "- p55.162: Vertical integral of water vapour\n",
    "\n",
    "\n",
    "First, we load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8a8d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, PredefinedSplit, GridSearchCV, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from sklearn.dummy import DummyRegressor\n",
    "import time\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39f6bdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wind_ava = pd.read_csv(\"C:\\\\Users\\\\victoria\\\\Escritorio\\\\MASTER\\\\Big Data Intelligence\\\\Assignment_1\\\\Big_Data_Intelligence\\\\wind_available.csv.gzip\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a9e684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pongo aqui abajo mi path \n",
    "\n",
    "wind_ava = pd.read_csv(\"/Users/pameladiaz/Desktop/GitHub/Big_Data_Intelligence/wind_available.csv.gzip\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b98d0c",
   "metadata": {},
   "source": [
    "## 1. Exploratory Data Analysis\n",
    "\n",
    "It is important before starting any machine learning project to observe and understand the data we have. Therefore, during this section we will be exploring how many features and how many instances the dataset has, which variables are categorical / numerical, which features have missing values and how many, whether there are constant columns, some statistics, and whether it is a regression or classification problem. \n",
    "\n",
    "Finally, we will plot some of the variables in terms of the target for trying to find some relations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e17857",
   "metadata": {},
   "source": [
    "### 1.1. Number of features\n",
    "\n",
    "As previosly explained, the dataset contains 555 variables and 4747 instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa66419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "wind_ava"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c4c01a",
   "metadata": {},
   "source": [
    "### 1.2. Variable type\n",
    "\n",
    "All the variables we are dealing with are numerical, most of them continuous and only the ones specifiying the time when the measure was taken (year, month, day and hour) are of type integer. This leaves us with a regression problem, as the target variable 'energy' is a continuous numerical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90498a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "wind_ava.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f0ccf6",
   "metadata": {},
   "source": [
    "### 1.3. Missing values\n",
    "\n",
    "If any of the variables has many missing values (more than 80%) we should remove it. After checking it, we are sure none of them  has more than 80% nulls, the maximum is 20.3 % for the variable lai_hv.13 (Leaf area index, high vegetation, section 13, Sotavento). It is difficult to determine the reason most of the variables has nulls, as we are not experts on the field. We will try to solve this later on the preprocessing with imputation methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51110864",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_percentage = wind_ava.isnull().sum()/ len(wind_ava) * 100\n",
    "print(null_percentage[null_percentage > 80])\n",
    "print(max(null_percentage))\n",
    "null_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cbe473",
   "metadata": {},
   "source": [
    "### 1.4. Statistics\n",
    "\n",
    "As all of our variables are numeric, we can perform a statistical analysis by giving some information about the mean, standard deviation, quantiles etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68935e6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wind_ava.describe().T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058db2fc",
   "metadata": {},
   "source": [
    "### 1.5. Constant columns\n",
    "\n",
    "Similarly with the null values, if any column has constant values, this gives no information to the final model, so it should be removed. Taking into account that some variables have null values, we should look if the number of unique values is less or equal to 2 (the constant value and Nan). As is not the case, we don't have any column with constant values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dfc856",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(wind_ava.nunique() <= 2)\n",
    "#wind_ava.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b51b23b",
   "metadata": {},
   "source": [
    "### 1.6. Visualization\n",
    "\n",
    "Let's observe first how the objective variable looks. It follows an exponential distribution, as most of the values are around the highest energy production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170871c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_rows', 10)\n",
    "\n",
    "plt.hist(wind_ava['energy'],  bins=30, color='skyblue', edgecolor='black')\n",
    "plt.title(f'Energy histogram')\n",
    "plt.xlabel('Energy value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Muestra el histograma\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3587ec38",
   "metadata": {},
   "source": [
    "We could also observe the mean enery production per year. As observed,  most of the years have similar energy production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94431d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el promedio de energía por año\n",
    "mean_energy = wind_ava.groupby('year')['energy'].mean()\n",
    "\n",
    "# Crear un gráfico de barras\n",
    "plt.figure(figsize=(10, 5))  # Ajusta el tamaño del gráfico según tus preferencias\n",
    "mean_energy.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "\n",
    "# Personalizar el gráfico (opcional)\n",
    "plt.title('Energy produced per year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Energy')\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Mostrar el gráfico de barras\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3436a8",
   "metadata": {},
   "source": [
    "Similarly, we could observe the mean energy production per month. This plot give us more valuable information, becuase, as it could be expected, on the months of worst weather and more wind, the energy production is highly increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3d9bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el promedio de energía por año\n",
    "mean_energy = wind_ava.groupby('month')['energy'].mean()\n",
    "\n",
    "# Crear un gráfico de barras\n",
    "plt.figure(figsize=(10, 5))  # Ajusta el tamaño del gráfico según tus preferencias\n",
    "mean_energy.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "\n",
    "# Personalizar el gráfico (opcional)\n",
    "plt.title('Mean energy produced per month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Energy')\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Mostrar el gráfico de barras\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b72516",
   "metadata": {},
   "source": [
    "### 1.7. Feature transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68ce285",
   "metadata": {},
   "source": [
    "Althought we have many variables, it is understandable that the variables u100 (100 metre U wind component) and v100 (100 metre V wind component) give us the most valuable information. However, the modullus is more valuable, as measures the wind speed at 100 meters high. Therefore, we would add this as a new variable to our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86e5ee4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>energy</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>p54.162.1</th>\n",
       "      <th>p54.162.2</th>\n",
       "      <th>p54.162.3</th>\n",
       "      <th>p54.162.4</th>\n",
       "      <th>p54.162.5</th>\n",
       "      <th>...</th>\n",
       "      <th>wind_speed16</th>\n",
       "      <th>wind_speed17</th>\n",
       "      <th>wind_speed18</th>\n",
       "      <th>wind_speed19</th>\n",
       "      <th>wind_speed20</th>\n",
       "      <th>wind_speed21</th>\n",
       "      <th>wind_speed22</th>\n",
       "      <th>wind_speed23</th>\n",
       "      <th>wind_speed24</th>\n",
       "      <th>wind_speed25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>402.71</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>2.534970e+06</td>\n",
       "      <td>2.526864e+06</td>\n",
       "      <td>2.518754e+06</td>\n",
       "      <td>2.510648e+06</td>\n",
       "      <td>2.502537e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>5.134636</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.393808</td>\n",
       "      <td>5.137268</td>\n",
       "      <td>4.948249</td>\n",
       "      <td>4.760744</td>\n",
       "      <td>4.574870</td>\n",
       "      <td>4.389944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>696.80</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.521184e+06</td>\n",
       "      <td>2.513088e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.298553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.869726</td>\n",
       "      <td>4.655077</td>\n",
       "      <td>4.440835</td>\n",
       "      <td>5.300003</td>\n",
       "      <td>5.085998</td>\n",
       "      <td>4.872320</td>\n",
       "      <td>4.658335</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1591.15</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2.533727e+06</td>\n",
       "      <td>2.525703e+06</td>\n",
       "      <td>2.517678e+06</td>\n",
       "      <td>2.509654e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.587899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.529130</td>\n",
       "      <td>5.324473</td>\n",
       "      <td>5.122190</td>\n",
       "      <td>4.922326</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1338.62</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.526548e+06</td>\n",
       "      <td>2.518609e+06</td>\n",
       "      <td>2.510670e+06</td>\n",
       "      <td>2.502732e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>4.779456</td>\n",
       "      <td>4.715311</td>\n",
       "      <td>4.655777</td>\n",
       "      <td>4.600019</td>\n",
       "      <td>4.549180</td>\n",
       "      <td>4.698198</td>\n",
       "      <td>4.634036</td>\n",
       "      <td>4.573135</td>\n",
       "      <td>4.516475</td>\n",
       "      <td>4.463542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>562.50</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>2.529543e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.513702e+06</td>\n",
       "      <td>2.505782e+06</td>\n",
       "      <td>2.497861e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>3.941046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.917355</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.851946</td>\n",
       "      <td>3.843097</td>\n",
       "      <td>3.836565</td>\n",
       "      <td>3.832704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 580 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    energy  year  month  day  hour     p54.162.1     p54.162.2     p54.162.3  \\\n",
       "0   402.71  2005      1    2    18  2.534970e+06  2.526864e+06  2.518754e+06   \n",
       "1   696.80  2005      1    3     0           NaN           NaN  2.521184e+06   \n",
       "2  1591.15  2005      1    3     6  2.533727e+06  2.525703e+06  2.517678e+06   \n",
       "3  1338.62  2005      1    3    12           NaN  2.526548e+06  2.518609e+06   \n",
       "4   562.50  2005      1    3    18  2.529543e+06           NaN  2.513702e+06   \n",
       "\n",
       "      p54.162.4     p54.162.5  ...  wind_speed16  wind_speed17  wind_speed18  \\\n",
       "0  2.510648e+06  2.502537e+06  ...      5.134636           NaN           NaN   \n",
       "1  2.513088e+06           NaN  ...      5.298553           NaN      4.869726   \n",
       "2  2.509654e+06           NaN  ...      5.587899           NaN           NaN   \n",
       "3  2.510670e+06  2.502732e+06  ...      4.779456      4.715311      4.655777   \n",
       "4  2.505782e+06  2.497861e+06  ...      3.941046           NaN           NaN   \n",
       "\n",
       "   wind_speed19  wind_speed20  wind_speed21  wind_speed22  wind_speed23  \\\n",
       "0           NaN      4.393808      5.137268      4.948249      4.760744   \n",
       "1      4.655077      4.440835      5.300003      5.085998      4.872320   \n",
       "2           NaN           NaN      5.529130      5.324473      5.122190   \n",
       "3      4.600019      4.549180      4.698198      4.634036      4.573135   \n",
       "4      3.917355           NaN           NaN      3.851946      3.843097   \n",
       "\n",
       "   wind_speed24  wind_speed25  \n",
       "0      4.574870      4.389944  \n",
       "1      4.658335           NaN  \n",
       "2      4.922326           NaN  \n",
       "3      4.516475      4.463542  \n",
       "4      3.836565      3.832704  \n",
       "\n",
       "[5 rows x 580 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## CHAT GPT\n",
    "# Assuming wind_ava is your DataFrame\n",
    "v_wind_columns = wind_ava.columns[wind_ava.columns.str.startswith('v100.')]\n",
    "u_wind_columns = wind_ava.columns[wind_ava.columns.str.startswith('u100.')]\n",
    "\n",
    "# Iterate through pairs of u and v columns\n",
    "for u_col, v_col in zip(u_wind_columns, v_wind_columns):\n",
    "    # Extract u and v components\n",
    "    u_component = wind_ava[u_col]\n",
    "    v_component = wind_ava[v_col]\n",
    "\n",
    "    # Calculate wind speed\n",
    "    wind_speed_col = f\"wind_speed{u_col.split('.')[1]}\"\n",
    "    wind_ava[wind_speed_col] = np.sqrt(u_component**2 + v_component**2)\n",
    "\n",
    "# Display the modified DataFrame with new wind speed columns\n",
    "wind_ava.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3581b4c",
   "metadata": {},
   "source": [
    "As can be observed, this new variable againts the response variable follows a linear relation, so it will give us valuable information when modelling the predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d79fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Obtain the columns defining the u and v components of the wind \n",
    "wind_speed_columns = wind_ava.columns[wind_ava.columns.str.startswith('wind_speed')]\n",
    "\n",
    " \n",
    "plt.scatter(wind_ava['energy'], wind_ava[wind_speed_columns].mean(axis=1), c='skyblue')\n",
    "plt.xlabel('Energy')\n",
    "plt.ylabel('Mean wind speed')\n",
    "plt.title('U and V wind components vs Energy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571dd698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "042a7871",
   "metadata": {},
   "source": [
    "## 2. Inner, outer evaluation and metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded8192d",
   "metadata": {},
   "source": [
    "### 2.1. Holdout split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c768dc42",
   "metadata": {},
   "source": [
    "After a exploration of our dataset, the next crucial step was to select the evaluation strategy. Given the considerable volume of data at our disposal, we decided to perform a holdout strategy for both inner and outer evaluations. Cross-validation was not used because it is more computationally intensive and time-consuming, particularly in the context of our substantial dataset. \n",
    "\n",
    "Moreover, a key factor influencing our choice is the intrinsic nature of our data, a time series with dependencies that render it non-independent and non-identically distributed (non-i.i.d.). Consequently, we have employed a partitioning strategy based on complete years. This approach ensures that each partition is not only representative of the problem at hand but also respects the temporal dependencies inherent in the data.\n",
    "\n",
    "Visualizing the dataset through plots further informed our decision-making. Notably, for the year 2008, data is available only for January and February, whereas in 2009, we have a dataset spanning from March to December. To ensure a representative evaluation, we opted to combine the data from these two years, designating them as the test partition. The remaining years were used for training.\n",
    "\n",
    "Later on, within our training data, we took another step and split it into two parts: train-validation and train-train. We used the data from 2007 for train-validation, while 2005 and 2006 became the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60b4f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "plt.bar(wind_ava['year'].value_counts().index, wind_ava['year'].value_counts().values, color='skyblue', edgecolor='black')\n",
    "plt.title(f'Year barplot')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Muestra el histograma\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef69791",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # Optional, but can enhance the appearance of the plot\n",
    "\n",
    "# Assuming 'wind_ava' is your DataFrame with 'year' and 'month' columns\n",
    "\n",
    "# Get unique years in the DataFrame\n",
    "unique_years = wind_ava['year'].unique()\n",
    "\n",
    "# Set up subplots\n",
    "fig, axes = plt.subplots(nrows=len(unique_years), ncols=1, figsize=(8, 5 * len(unique_years)))\n",
    "\n",
    "# Loop through each year and create a bar plot for the frequency of each month\n",
    "for i, year in enumerate(unique_years):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Filter data for the current year\n",
    "    data_year = wind_ava[wind_ava['year'] == year]\n",
    "    \n",
    "    # Create a count plot for the 'month' column\n",
    "    sns.countplot(x='month', data=data_year, ax=ax, palette='viridis')\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_title(f'Monthly Frequency for the Year {year}')\n",
    "    ax.set_xlabel('Month')\n",
    "    ax.set_ylabel('Frequency')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3fd0c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##chat gpt\n",
    "# Splitting the data into training and test sets\n",
    "train_data = wind_ava[wind_ava['year'].isin([2005, 2006, 2007])]\n",
    "test_data = wind_ava[wind_ava['year'].isin([2008, 2009])]\n",
    "\n",
    "# Further split the training set into train-train and train-validation\n",
    "train_train_data = train_data[train_data['year'].isin([2005, 2006])]\n",
    "train_validation_data = train_data[train_data['year'] == 2007]\n",
    "\n",
    "\n",
    "# Features and target for training set\n",
    "X_train_train = train_train_data.drop('energy', axis=1)\n",
    "#y_train_train = train_train_data['energy']\n",
    "\n",
    "X_train = train_data.drop('energy', axis=1)\n",
    "y_train = train_data['energy']\n",
    "\n",
    "\n",
    "# Features and target for validation set\n",
    "X_train_validation = train_validation_data.drop('energy', axis=1)\n",
    "#y_train_validation = train_validation_data['energy']\n",
    "\n",
    "# Features and target for test set\n",
    "X_test = test_data.drop('energy', axis=1)\n",
    "y_test = test_data['energy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7968af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#X_train_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd1d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc34a5c",
   "metadata": {},
   "source": [
    "### 2.2. Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba00b51",
   "metadata": {},
   "source": [
    "CHAT GPT:\n",
    "The choice of which metric to use for evaluating a regression model depends on the specific characteristics of the problem and our goals. As a summary of the benefits and drawbacks of the most commonly used regression metrics:\n",
    "\n",
    "- Mean Squared Error (MSE) or Root Mean Squared Error (RMSE): These are often preferred when large errors should be penalized more heavily, and the distribution of errors is approximately normal. However, if your data contains outliers, MSE and RMSE can be significantly affected.\n",
    "\n",
    "- Mean Absolute Error (MAE): These are more robust to outliers. If your dataset has a skewed distribution or contains outliers, MAE might be more appropriate.\n",
    "\n",
    "Therefore, we will evaluate the outliers of our objetive variable to understand which metric is better on this problem. As observed, the data is rigth skewed, with many outliers on the rigth section, therefore, a MAE metric would affect less to these errors when predicting outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0847e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(train_train_data['energy'], vert=False, widths=0.7, patch_artist=True, boxprops=dict(facecolor='skyblue', color='black'))\n",
    "plt.title('Energy Boxplot')\n",
    "plt.xlabel('Energy value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526d21e4-bc5b-491c-a1cf-8ff78a60ee2d",
   "metadata": {},
   "source": [
    "### 2.3. Strategy\n",
    "\n",
    "Issues to address: \n",
    "- Feature selection: are all the 550 attributes really necessary? Maybe only the attributes related to the Sotavento location (13th location in the grid) are actually required? Or only the attributes related to wind? \r",
    "- Imputation: Does imputation improve performance in this problem? Which method seem to work best?- Modeling:s) Which method is more appropriate to the proble). Does hyper-parameter tuning contribute to improve performance? At what cost (compute tim?). Which HPO method does perform bette\n",
    "\n",
    "Strategy:\n",
    "\n",
    "The idea is to cover the first two issues on the preprocessing on the data, starting first with the feature selection in order to reduce the data size as much as posible for making the later computations easier. During this preprocessing, on each stage we compute a pipeline where we perform the necesary transformations (defining the adequate search space on each stage), and later we predict with a KNN. This method was chosen due to the simplicity and the small number of hyperparamters it has (it was used on default mode, not HPO of KNN during the preprocessing). Appart from feature selection and imputation, also scaling hyperparameters will be tunned during this process. All this tuning will be donde by performing a GridSearch over the hyperparameters in order to be more exhaustive, as the process of preprocessing is the most important in the project. When preprocessing is finished, we will apply all the necessary transformations to the data for having it ready for the modeling.\n",
    "\n",
    "Next step is to evaluate different models/methods for the problem. First, we would define a naive model with the median of the data as prediction. Secondly, we would address four different methods: Trees, KNN, gradient and boosting. All of them would be modelled both default and with HPO. Among HPO, it is difficult to try all the possible methods, so we have chosen to try Random Search, Grid Search, Bayesian Search, and Succesive Halving. All the metric results and time measurements would be kept for later comparison.\n",
    "\n",
    "New issue: )."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d87236f",
   "metadata": {},
   "source": [
    "## 3. Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c30737",
   "metadata": {},
   "source": [
    "### 3.1. Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cd7a7d-7402-4650-b164-fd667b4c39ba",
   "metadata": {},
   "source": [
    "Explicar aqui que no todas las features son necesarias y que las eligiremos en función de dos métricas y 5 rangos de número de features seleccionadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec9267c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model MAE: 314.9798252957234\n"
     ]
    }
   ],
   "source": [
    "#We set a seed for reproducibility, althougth the split is predefined, some steps in all the pipelines defined may have some randomness\n",
    "random_state= 100516919\n",
    "\n",
    "# Load a sample dataset for demonstration\n",
    "X = X_train\n",
    "y = y_train\n",
    "\n",
    "# Define the holdout split\n",
    "indices= ([-1] * (len(X_train_train))) + ([0] * (len(X_train_validation)))\n",
    "inner= PredefinedSplit(indices)\n",
    "\n",
    "pipeline_feature_selection = Pipeline([\n",
    "    ('imputer', SimpleImputer()), \n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_selection', SelectKBest()),  # Placeholder, will be replaced during grid search\n",
    "    ('knn', KNeighborsRegressor(n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Define the hyperparameter grid for GridSearchCV\n",
    "param_grid_fs = {\n",
    "    'feature_selection__score_func': [f_regression, mutual_info_regression],\n",
    "    'feature_selection__k': [100,150,250,300,350]  # Adjust as needed\n",
    "}\n",
    "\n",
    "\n",
    "# Use RandomizedSearchCV to find the best combination of parameters\n",
    "grid_search = RandomizedSearchCV(pipeline_feature_selection, param_grid_fs, cv=inner, scoring='neg_mean_absolute_error', n_jobs=-1, random_state= random_state )\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate and print the Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Best Model MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e64e6a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'feature_selection__score_func': <function mutual_info_regression at 0x13f757310>, 'feature_selection__k': 250}\n"
     ]
    }
   ],
   "source": [
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f062b908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of selected features: [ 79  80  81  82  83  84  85  87  89  90  91  93  94  96 102 103 154 155\n",
      " 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173\n",
      " 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191\n",
      " 192 193 194 195 196 197 198 200 201 202 203 254 255 256 257 258 259 260\n",
      " 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278\n",
      " 279 280 281 282 283 284 285 286 287 288 289 290 291 292 294 295 296 297\n",
      " 298 299 300 301 302 303 379 380 381 382 383 384 385 386 387 388 389 390\n",
      " 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408\n",
      " 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426\n",
      " 427 428 479 480 481 482 484 485 486 487 488 489 490 504 505 506 507 508\n",
      " 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526\n",
      " 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544\n",
      " 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562\n",
      " 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578]\n"
     ]
    }
   ],
   "source": [
    "# Access the selected features\n",
    "selected_features = best_model.named_steps['feature_selection'].get_support()\n",
    "\n",
    "# Print the indices of selected features\n",
    "selected_feature_indices = np.where(selected_features)[0]\n",
    "print(f\"Indices of selected features: {selected_feature_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bc8a9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Label: year, Numerical Index: 0\n",
      "Column Label: month, Numerical Index: 1\n",
      "Column Label: day, Numerical Index: 2\n",
      "Column Label: hour, Numerical Index: 3\n",
      "Column Label: p54.162.1, Numerical Index: 4\n",
      "Column Label: p54.162.2, Numerical Index: 5\n",
      "Column Label: p54.162.3, Numerical Index: 6\n",
      "Column Label: p54.162.4, Numerical Index: 7\n",
      "Column Label: p54.162.5, Numerical Index: 8\n",
      "Column Label: p54.162.6, Numerical Index: 9\n",
      "Column Label: p54.162.7, Numerical Index: 10\n",
      "Column Label: p54.162.8, Numerical Index: 11\n",
      "Column Label: p54.162.9, Numerical Index: 12\n",
      "Column Label: p54.162.10, Numerical Index: 13\n",
      "Column Label: p54.162.11, Numerical Index: 14\n",
      "Column Label: p54.162.12, Numerical Index: 15\n",
      "Column Label: p54.162.13, Numerical Index: 16\n",
      "Column Label: p54.162.14, Numerical Index: 17\n",
      "Column Label: p54.162.15, Numerical Index: 18\n",
      "Column Label: p54.162.16, Numerical Index: 19\n",
      "Column Label: p54.162.17, Numerical Index: 20\n",
      "Column Label: p54.162.18, Numerical Index: 21\n",
      "Column Label: p54.162.19, Numerical Index: 22\n",
      "Column Label: p54.162.20, Numerical Index: 23\n",
      "Column Label: p54.162.21, Numerical Index: 24\n",
      "Column Label: p54.162.22, Numerical Index: 25\n",
      "Column Label: p54.162.23, Numerical Index: 26\n",
      "Column Label: p54.162.24, Numerical Index: 27\n",
      "Column Label: p54.162.25, Numerical Index: 28\n",
      "Column Label: p55.162.1, Numerical Index: 29\n",
      "Column Label: p55.162.2, Numerical Index: 30\n",
      "Column Label: p55.162.3, Numerical Index: 31\n",
      "Column Label: p55.162.4, Numerical Index: 32\n",
      "Column Label: p55.162.5, Numerical Index: 33\n",
      "Column Label: p55.162.6, Numerical Index: 34\n",
      "Column Label: p55.162.7, Numerical Index: 35\n",
      "Column Label: p55.162.8, Numerical Index: 36\n",
      "Column Label: p55.162.9, Numerical Index: 37\n",
      "Column Label: p55.162.10, Numerical Index: 38\n",
      "Column Label: p55.162.11, Numerical Index: 39\n",
      "Column Label: p55.162.12, Numerical Index: 40\n",
      "Column Label: p55.162.13, Numerical Index: 41\n",
      "Column Label: p55.162.14, Numerical Index: 42\n",
      "Column Label: p55.162.15, Numerical Index: 43\n",
      "Column Label: p55.162.16, Numerical Index: 44\n",
      "Column Label: p55.162.17, Numerical Index: 45\n",
      "Column Label: p55.162.18, Numerical Index: 46\n",
      "Column Label: p55.162.19, Numerical Index: 47\n",
      "Column Label: p55.162.20, Numerical Index: 48\n",
      "Column Label: p55.162.21, Numerical Index: 49\n",
      "Column Label: p55.162.22, Numerical Index: 50\n",
      "Column Label: p55.162.23, Numerical Index: 51\n",
      "Column Label: p55.162.24, Numerical Index: 52\n",
      "Column Label: p55.162.25, Numerical Index: 53\n",
      "Column Label: cape.1, Numerical Index: 54\n",
      "Column Label: cape.2, Numerical Index: 55\n",
      "Column Label: cape.3, Numerical Index: 56\n",
      "Column Label: cape.4, Numerical Index: 57\n",
      "Column Label: cape.5, Numerical Index: 58\n",
      "Column Label: cape.6, Numerical Index: 59\n",
      "Column Label: cape.7, Numerical Index: 60\n",
      "Column Label: cape.8, Numerical Index: 61\n",
      "Column Label: cape.9, Numerical Index: 62\n",
      "Column Label: cape.10, Numerical Index: 63\n",
      "Column Label: cape.11, Numerical Index: 64\n",
      "Column Label: cape.12, Numerical Index: 65\n",
      "Column Label: cape.13, Numerical Index: 66\n",
      "Column Label: cape.14, Numerical Index: 67\n",
      "Column Label: cape.15, Numerical Index: 68\n",
      "Column Label: cape.16, Numerical Index: 69\n",
      "Column Label: cape.17, Numerical Index: 70\n",
      "Column Label: cape.18, Numerical Index: 71\n",
      "Column Label: cape.19, Numerical Index: 72\n",
      "Column Label: cape.20, Numerical Index: 73\n",
      "Column Label: cape.21, Numerical Index: 74\n",
      "Column Label: cape.22, Numerical Index: 75\n",
      "Column Label: cape.23, Numerical Index: 76\n",
      "Column Label: cape.24, Numerical Index: 77\n",
      "Column Label: cape.25, Numerical Index: 78\n",
      "Column Label: p59.162.1, Numerical Index: 79\n",
      "Column Label: p59.162.2, Numerical Index: 80\n",
      "Column Label: p59.162.3, Numerical Index: 81\n",
      "Column Label: p59.162.4, Numerical Index: 82\n",
      "Column Label: p59.162.5, Numerical Index: 83\n",
      "Column Label: p59.162.6, Numerical Index: 84\n",
      "Column Label: p59.162.7, Numerical Index: 85\n",
      "Column Label: p59.162.8, Numerical Index: 86\n",
      "Column Label: p59.162.9, Numerical Index: 87\n",
      "Column Label: p59.162.10, Numerical Index: 88\n",
      "Column Label: p59.162.11, Numerical Index: 89\n",
      "Column Label: p59.162.12, Numerical Index: 90\n",
      "Column Label: p59.162.13, Numerical Index: 91\n",
      "Column Label: p59.162.14, Numerical Index: 92\n",
      "Column Label: p59.162.15, Numerical Index: 93\n",
      "Column Label: p59.162.16, Numerical Index: 94\n",
      "Column Label: p59.162.17, Numerical Index: 95\n",
      "Column Label: p59.162.18, Numerical Index: 96\n",
      "Column Label: p59.162.19, Numerical Index: 97\n",
      "Column Label: p59.162.20, Numerical Index: 98\n",
      "Column Label: p59.162.21, Numerical Index: 99\n",
      "Column Label: p59.162.22, Numerical Index: 100\n",
      "Column Label: p59.162.23, Numerical Index: 101\n",
      "Column Label: p59.162.24, Numerical Index: 102\n",
      "Column Label: p59.162.25, Numerical Index: 103\n",
      "Column Label: lai_lv.1, Numerical Index: 104\n",
      "Column Label: lai_lv.2, Numerical Index: 105\n",
      "Column Label: lai_lv.3, Numerical Index: 106\n",
      "Column Label: lai_lv.4, Numerical Index: 107\n",
      "Column Label: lai_lv.5, Numerical Index: 108\n",
      "Column Label: lai_lv.6, Numerical Index: 109\n",
      "Column Label: lai_lv.7, Numerical Index: 110\n",
      "Column Label: lai_lv.8, Numerical Index: 111\n",
      "Column Label: lai_lv.9, Numerical Index: 112\n",
      "Column Label: lai_lv.10, Numerical Index: 113\n",
      "Column Label: lai_lv.11, Numerical Index: 114\n",
      "Column Label: lai_lv.12, Numerical Index: 115\n",
      "Column Label: lai_lv.13, Numerical Index: 116\n",
      "Column Label: lai_lv.14, Numerical Index: 117\n",
      "Column Label: lai_lv.15, Numerical Index: 118\n",
      "Column Label: lai_lv.16, Numerical Index: 119\n",
      "Column Label: lai_lv.17, Numerical Index: 120\n",
      "Column Label: lai_lv.18, Numerical Index: 121\n",
      "Column Label: lai_lv.19, Numerical Index: 122\n",
      "Column Label: lai_lv.20, Numerical Index: 123\n",
      "Column Label: lai_lv.21, Numerical Index: 124\n",
      "Column Label: lai_lv.22, Numerical Index: 125\n",
      "Column Label: lai_lv.23, Numerical Index: 126\n",
      "Column Label: lai_lv.24, Numerical Index: 127\n",
      "Column Label: lai_lv.25, Numerical Index: 128\n",
      "Column Label: lai_hv.1, Numerical Index: 129\n",
      "Column Label: lai_hv.2, Numerical Index: 130\n",
      "Column Label: lai_hv.3, Numerical Index: 131\n",
      "Column Label: lai_hv.4, Numerical Index: 132\n",
      "Column Label: lai_hv.5, Numerical Index: 133\n",
      "Column Label: lai_hv.6, Numerical Index: 134\n",
      "Column Label: lai_hv.7, Numerical Index: 135\n",
      "Column Label: lai_hv.8, Numerical Index: 136\n",
      "Column Label: lai_hv.9, Numerical Index: 137\n",
      "Column Label: lai_hv.10, Numerical Index: 138\n",
      "Column Label: lai_hv.11, Numerical Index: 139\n",
      "Column Label: lai_hv.12, Numerical Index: 140\n",
      "Column Label: lai_hv.13, Numerical Index: 141\n",
      "Column Label: lai_hv.14, Numerical Index: 142\n",
      "Column Label: lai_hv.15, Numerical Index: 143\n",
      "Column Label: lai_hv.16, Numerical Index: 144\n",
      "Column Label: lai_hv.17, Numerical Index: 145\n",
      "Column Label: lai_hv.18, Numerical Index: 146\n",
      "Column Label: lai_hv.19, Numerical Index: 147\n",
      "Column Label: lai_hv.20, Numerical Index: 148\n",
      "Column Label: lai_hv.21, Numerical Index: 149\n",
      "Column Label: lai_hv.22, Numerical Index: 150\n",
      "Column Label: lai_hv.23, Numerical Index: 151\n",
      "Column Label: lai_hv.24, Numerical Index: 152\n",
      "Column Label: lai_hv.25, Numerical Index: 153\n",
      "Column Label: u10n.1, Numerical Index: 154\n",
      "Column Label: u10n.2, Numerical Index: 155\n",
      "Column Label: u10n.3, Numerical Index: 156\n",
      "Column Label: u10n.4, Numerical Index: 157\n",
      "Column Label: u10n.5, Numerical Index: 158\n",
      "Column Label: u10n.6, Numerical Index: 159\n",
      "Column Label: u10n.7, Numerical Index: 160\n",
      "Column Label: u10n.8, Numerical Index: 161\n",
      "Column Label: u10n.9, Numerical Index: 162\n",
      "Column Label: u10n.10, Numerical Index: 163\n",
      "Column Label: u10n.11, Numerical Index: 164\n",
      "Column Label: u10n.12, Numerical Index: 165\n",
      "Column Label: u10n.13, Numerical Index: 166\n",
      "Column Label: u10n.14, Numerical Index: 167\n",
      "Column Label: u10n.15, Numerical Index: 168\n",
      "Column Label: u10n.16, Numerical Index: 169\n",
      "Column Label: u10n.17, Numerical Index: 170\n",
      "Column Label: u10n.18, Numerical Index: 171\n",
      "Column Label: u10n.19, Numerical Index: 172\n",
      "Column Label: u10n.20, Numerical Index: 173\n",
      "Column Label: u10n.21, Numerical Index: 174\n",
      "Column Label: u10n.22, Numerical Index: 175\n",
      "Column Label: u10n.23, Numerical Index: 176\n",
      "Column Label: u10n.24, Numerical Index: 177\n",
      "Column Label: u10n.25, Numerical Index: 178\n",
      "Column Label: v10n.1, Numerical Index: 179\n",
      "Column Label: v10n.2, Numerical Index: 180\n",
      "Column Label: v10n.3, Numerical Index: 181\n",
      "Column Label: v10n.4, Numerical Index: 182\n",
      "Column Label: v10n.5, Numerical Index: 183\n",
      "Column Label: v10n.6, Numerical Index: 184\n",
      "Column Label: v10n.7, Numerical Index: 185\n",
      "Column Label: v10n.8, Numerical Index: 186\n",
      "Column Label: v10n.9, Numerical Index: 187\n",
      "Column Label: v10n.10, Numerical Index: 188\n",
      "Column Label: v10n.11, Numerical Index: 189\n",
      "Column Label: v10n.12, Numerical Index: 190\n",
      "Column Label: v10n.13, Numerical Index: 191\n",
      "Column Label: v10n.14, Numerical Index: 192\n",
      "Column Label: v10n.15, Numerical Index: 193\n",
      "Column Label: v10n.16, Numerical Index: 194\n",
      "Column Label: v10n.17, Numerical Index: 195\n",
      "Column Label: v10n.18, Numerical Index: 196\n",
      "Column Label: v10n.19, Numerical Index: 197\n",
      "Column Label: v10n.20, Numerical Index: 198\n",
      "Column Label: v10n.21, Numerical Index: 199\n",
      "Column Label: v10n.22, Numerical Index: 200\n",
      "Column Label: v10n.23, Numerical Index: 201\n",
      "Column Label: v10n.24, Numerical Index: 202\n",
      "Column Label: v10n.25, Numerical Index: 203\n",
      "Column Label: sp.1, Numerical Index: 204\n",
      "Column Label: sp.2, Numerical Index: 205\n",
      "Column Label: sp.3, Numerical Index: 206\n",
      "Column Label: sp.4, Numerical Index: 207\n",
      "Column Label: sp.5, Numerical Index: 208\n",
      "Column Label: sp.6, Numerical Index: 209\n",
      "Column Label: sp.7, Numerical Index: 210\n",
      "Column Label: sp.8, Numerical Index: 211\n",
      "Column Label: sp.9, Numerical Index: 212\n",
      "Column Label: sp.10, Numerical Index: 213\n",
      "Column Label: sp.11, Numerical Index: 214\n",
      "Column Label: sp.12, Numerical Index: 215\n",
      "Column Label: sp.13, Numerical Index: 216\n",
      "Column Label: sp.14, Numerical Index: 217\n",
      "Column Label: sp.15, Numerical Index: 218\n",
      "Column Label: sp.16, Numerical Index: 219\n",
      "Column Label: sp.17, Numerical Index: 220\n",
      "Column Label: sp.18, Numerical Index: 221\n",
      "Column Label: sp.19, Numerical Index: 222\n",
      "Column Label: sp.20, Numerical Index: 223\n",
      "Column Label: sp.21, Numerical Index: 224\n",
      "Column Label: sp.22, Numerical Index: 225\n",
      "Column Label: sp.23, Numerical Index: 226\n",
      "Column Label: sp.24, Numerical Index: 227\n",
      "Column Label: sp.25, Numerical Index: 228\n",
      "Column Label: stl1.1, Numerical Index: 229\n",
      "Column Label: stl1.2, Numerical Index: 230\n",
      "Column Label: stl1.3, Numerical Index: 231\n",
      "Column Label: stl1.4, Numerical Index: 232\n",
      "Column Label: stl1.5, Numerical Index: 233\n",
      "Column Label: stl1.6, Numerical Index: 234\n",
      "Column Label: stl1.7, Numerical Index: 235\n",
      "Column Label: stl1.8, Numerical Index: 236\n",
      "Column Label: stl1.9, Numerical Index: 237\n",
      "Column Label: stl1.10, Numerical Index: 238\n",
      "Column Label: stl1.11, Numerical Index: 239\n",
      "Column Label: stl1.12, Numerical Index: 240\n",
      "Column Label: stl1.13, Numerical Index: 241\n",
      "Column Label: stl1.14, Numerical Index: 242\n",
      "Column Label: stl1.15, Numerical Index: 243\n",
      "Column Label: stl1.16, Numerical Index: 244\n",
      "Column Label: stl1.17, Numerical Index: 245\n",
      "Column Label: stl1.18, Numerical Index: 246\n",
      "Column Label: stl1.19, Numerical Index: 247\n",
      "Column Label: stl1.20, Numerical Index: 248\n",
      "Column Label: stl1.21, Numerical Index: 249\n",
      "Column Label: stl1.22, Numerical Index: 250\n",
      "Column Label: stl1.23, Numerical Index: 251\n",
      "Column Label: stl1.24, Numerical Index: 252\n",
      "Column Label: stl1.25, Numerical Index: 253\n",
      "Column Label: u10.1, Numerical Index: 254\n",
      "Column Label: u10.2, Numerical Index: 255\n",
      "Column Label: u10.3, Numerical Index: 256\n",
      "Column Label: u10.4, Numerical Index: 257\n",
      "Column Label: u10.5, Numerical Index: 258\n",
      "Column Label: u10.6, Numerical Index: 259\n",
      "Column Label: u10.7, Numerical Index: 260\n",
      "Column Label: u10.8, Numerical Index: 261\n",
      "Column Label: u10.9, Numerical Index: 262\n",
      "Column Label: u10.10, Numerical Index: 263\n",
      "Column Label: u10.11, Numerical Index: 264\n",
      "Column Label: u10.12, Numerical Index: 265\n",
      "Column Label: u10.13, Numerical Index: 266\n",
      "Column Label: u10.14, Numerical Index: 267\n",
      "Column Label: u10.15, Numerical Index: 268\n",
      "Column Label: u10.16, Numerical Index: 269\n",
      "Column Label: u10.17, Numerical Index: 270\n",
      "Column Label: u10.18, Numerical Index: 271\n",
      "Column Label: u10.19, Numerical Index: 272\n",
      "Column Label: u10.20, Numerical Index: 273\n",
      "Column Label: u10.21, Numerical Index: 274\n",
      "Column Label: u10.22, Numerical Index: 275\n",
      "Column Label: u10.23, Numerical Index: 276\n",
      "Column Label: u10.24, Numerical Index: 277\n",
      "Column Label: u10.25, Numerical Index: 278\n",
      "Column Label: v10.1, Numerical Index: 279\n",
      "Column Label: v10.2, Numerical Index: 280\n",
      "Column Label: v10.3, Numerical Index: 281\n",
      "Column Label: v10.4, Numerical Index: 282\n",
      "Column Label: v10.5, Numerical Index: 283\n",
      "Column Label: v10.6, Numerical Index: 284\n",
      "Column Label: v10.7, Numerical Index: 285\n",
      "Column Label: v10.8, Numerical Index: 286\n",
      "Column Label: v10.9, Numerical Index: 287\n",
      "Column Label: v10.10, Numerical Index: 288\n",
      "Column Label: v10.11, Numerical Index: 289\n",
      "Column Label: v10.12, Numerical Index: 290\n",
      "Column Label: v10.13, Numerical Index: 291\n",
      "Column Label: v10.14, Numerical Index: 292\n",
      "Column Label: v10.15, Numerical Index: 293\n",
      "Column Label: v10.16, Numerical Index: 294\n",
      "Column Label: v10.17, Numerical Index: 295\n",
      "Column Label: v10.18, Numerical Index: 296\n",
      "Column Label: v10.19, Numerical Index: 297\n",
      "Column Label: v10.20, Numerical Index: 298\n",
      "Column Label: v10.21, Numerical Index: 299\n",
      "Column Label: v10.22, Numerical Index: 300\n",
      "Column Label: v10.23, Numerical Index: 301\n",
      "Column Label: v10.24, Numerical Index: 302\n",
      "Column Label: v10.25, Numerical Index: 303\n",
      "Column Label: t2m.1, Numerical Index: 304\n",
      "Column Label: t2m.2, Numerical Index: 305\n",
      "Column Label: t2m.3, Numerical Index: 306\n",
      "Column Label: t2m.4, Numerical Index: 307\n",
      "Column Label: t2m.5, Numerical Index: 308\n",
      "Column Label: t2m.6, Numerical Index: 309\n",
      "Column Label: t2m.7, Numerical Index: 310\n",
      "Column Label: t2m.8, Numerical Index: 311\n",
      "Column Label: t2m.9, Numerical Index: 312\n",
      "Column Label: t2m.10, Numerical Index: 313\n",
      "Column Label: t2m.11, Numerical Index: 314\n",
      "Column Label: t2m.12, Numerical Index: 315\n",
      "Column Label: t2m.13, Numerical Index: 316\n",
      "Column Label: t2m.14, Numerical Index: 317\n",
      "Column Label: t2m.15, Numerical Index: 318\n",
      "Column Label: t2m.16, Numerical Index: 319\n",
      "Column Label: t2m.17, Numerical Index: 320\n",
      "Column Label: t2m.18, Numerical Index: 321\n",
      "Column Label: t2m.19, Numerical Index: 322\n",
      "Column Label: t2m.20, Numerical Index: 323\n",
      "Column Label: t2m.21, Numerical Index: 324\n",
      "Column Label: t2m.22, Numerical Index: 325\n",
      "Column Label: t2m.23, Numerical Index: 326\n",
      "Column Label: t2m.24, Numerical Index: 327\n",
      "Column Label: t2m.25, Numerical Index: 328\n",
      "Column Label: stl2.1, Numerical Index: 329\n",
      "Column Label: stl2.2, Numerical Index: 330\n",
      "Column Label: stl2.3, Numerical Index: 331\n",
      "Column Label: stl2.4, Numerical Index: 332\n",
      "Column Label: stl2.5, Numerical Index: 333\n",
      "Column Label: stl2.6, Numerical Index: 334\n",
      "Column Label: stl2.7, Numerical Index: 335\n",
      "Column Label: stl2.8, Numerical Index: 336\n",
      "Column Label: stl2.9, Numerical Index: 337\n",
      "Column Label: stl2.10, Numerical Index: 338\n",
      "Column Label: stl2.11, Numerical Index: 339\n",
      "Column Label: stl2.12, Numerical Index: 340\n",
      "Column Label: stl2.13, Numerical Index: 341\n",
      "Column Label: stl2.14, Numerical Index: 342\n",
      "Column Label: stl2.15, Numerical Index: 343\n",
      "Column Label: stl2.16, Numerical Index: 344\n",
      "Column Label: stl2.17, Numerical Index: 345\n",
      "Column Label: stl2.18, Numerical Index: 346\n",
      "Column Label: stl2.19, Numerical Index: 347\n",
      "Column Label: stl2.20, Numerical Index: 348\n",
      "Column Label: stl2.21, Numerical Index: 349\n",
      "Column Label: stl2.22, Numerical Index: 350\n",
      "Column Label: stl2.23, Numerical Index: 351\n",
      "Column Label: stl2.24, Numerical Index: 352\n",
      "Column Label: stl2.25, Numerical Index: 353\n",
      "Column Label: stl3.1, Numerical Index: 354\n",
      "Column Label: stl3.2, Numerical Index: 355\n",
      "Column Label: stl3.3, Numerical Index: 356\n",
      "Column Label: stl3.4, Numerical Index: 357\n",
      "Column Label: stl3.5, Numerical Index: 358\n",
      "Column Label: stl3.6, Numerical Index: 359\n",
      "Column Label: stl3.7, Numerical Index: 360\n",
      "Column Label: stl3.8, Numerical Index: 361\n",
      "Column Label: stl3.9, Numerical Index: 362\n",
      "Column Label: stl3.10, Numerical Index: 363\n",
      "Column Label: stl3.11, Numerical Index: 364\n",
      "Column Label: stl3.12, Numerical Index: 365\n",
      "Column Label: stl3.13, Numerical Index: 366\n",
      "Column Label: stl3.14, Numerical Index: 367\n",
      "Column Label: stl3.15, Numerical Index: 368\n",
      "Column Label: stl3.16, Numerical Index: 369\n",
      "Column Label: stl3.17, Numerical Index: 370\n",
      "Column Label: stl3.18, Numerical Index: 371\n",
      "Column Label: stl3.19, Numerical Index: 372\n",
      "Column Label: stl3.20, Numerical Index: 373\n",
      "Column Label: stl3.21, Numerical Index: 374\n",
      "Column Label: stl3.22, Numerical Index: 375\n",
      "Column Label: stl3.23, Numerical Index: 376\n",
      "Column Label: stl3.24, Numerical Index: 377\n",
      "Column Label: stl3.25, Numerical Index: 378\n",
      "Column Label: iews.1, Numerical Index: 379\n",
      "Column Label: iews.2, Numerical Index: 380\n",
      "Column Label: iews.3, Numerical Index: 381\n",
      "Column Label: iews.4, Numerical Index: 382\n",
      "Column Label: iews.5, Numerical Index: 383\n",
      "Column Label: iews.6, Numerical Index: 384\n",
      "Column Label: iews.7, Numerical Index: 385\n",
      "Column Label: iews.8, Numerical Index: 386\n",
      "Column Label: iews.9, Numerical Index: 387\n",
      "Column Label: iews.10, Numerical Index: 388\n",
      "Column Label: iews.11, Numerical Index: 389\n",
      "Column Label: iews.12, Numerical Index: 390\n",
      "Column Label: iews.13, Numerical Index: 391\n",
      "Column Label: iews.14, Numerical Index: 392\n",
      "Column Label: iews.15, Numerical Index: 393\n",
      "Column Label: iews.16, Numerical Index: 394\n",
      "Column Label: iews.17, Numerical Index: 395\n",
      "Column Label: iews.18, Numerical Index: 396\n",
      "Column Label: iews.19, Numerical Index: 397\n",
      "Column Label: iews.20, Numerical Index: 398\n",
      "Column Label: iews.21, Numerical Index: 399\n",
      "Column Label: iews.22, Numerical Index: 400\n",
      "Column Label: iews.23, Numerical Index: 401\n",
      "Column Label: iews.24, Numerical Index: 402\n",
      "Column Label: iews.25, Numerical Index: 403\n",
      "Column Label: inss.1, Numerical Index: 404\n",
      "Column Label: inss.2, Numerical Index: 405\n",
      "Column Label: inss.3, Numerical Index: 406\n",
      "Column Label: inss.4, Numerical Index: 407\n",
      "Column Label: inss.5, Numerical Index: 408\n",
      "Column Label: inss.6, Numerical Index: 409\n",
      "Column Label: inss.7, Numerical Index: 410\n",
      "Column Label: inss.8, Numerical Index: 411\n",
      "Column Label: inss.9, Numerical Index: 412\n",
      "Column Label: inss.10, Numerical Index: 413\n",
      "Column Label: inss.11, Numerical Index: 414\n",
      "Column Label: inss.12, Numerical Index: 415\n",
      "Column Label: inss.13, Numerical Index: 416\n",
      "Column Label: inss.14, Numerical Index: 417\n",
      "Column Label: inss.15, Numerical Index: 418\n",
      "Column Label: inss.16, Numerical Index: 419\n",
      "Column Label: inss.17, Numerical Index: 420\n",
      "Column Label: inss.18, Numerical Index: 421\n",
      "Column Label: inss.19, Numerical Index: 422\n",
      "Column Label: inss.20, Numerical Index: 423\n",
      "Column Label: inss.21, Numerical Index: 424\n",
      "Column Label: inss.22, Numerical Index: 425\n",
      "Column Label: inss.23, Numerical Index: 426\n",
      "Column Label: inss.24, Numerical Index: 427\n",
      "Column Label: inss.25, Numerical Index: 428\n",
      "Column Label: stl4.1, Numerical Index: 429\n",
      "Column Label: stl4.2, Numerical Index: 430\n",
      "Column Label: stl4.3, Numerical Index: 431\n",
      "Column Label: stl4.4, Numerical Index: 432\n",
      "Column Label: stl4.5, Numerical Index: 433\n",
      "Column Label: stl4.6, Numerical Index: 434\n",
      "Column Label: stl4.7, Numerical Index: 435\n",
      "Column Label: stl4.8, Numerical Index: 436\n",
      "Column Label: stl4.9, Numerical Index: 437\n",
      "Column Label: stl4.10, Numerical Index: 438\n",
      "Column Label: stl4.11, Numerical Index: 439\n",
      "Column Label: stl4.12, Numerical Index: 440\n",
      "Column Label: stl4.13, Numerical Index: 441\n",
      "Column Label: stl4.14, Numerical Index: 442\n",
      "Column Label: stl4.15, Numerical Index: 443\n",
      "Column Label: stl4.16, Numerical Index: 444\n",
      "Column Label: stl4.17, Numerical Index: 445\n",
      "Column Label: stl4.18, Numerical Index: 446\n",
      "Column Label: stl4.19, Numerical Index: 447\n",
      "Column Label: stl4.20, Numerical Index: 448\n",
      "Column Label: stl4.21, Numerical Index: 449\n",
      "Column Label: stl4.22, Numerical Index: 450\n",
      "Column Label: stl4.23, Numerical Index: 451\n",
      "Column Label: stl4.24, Numerical Index: 452\n",
      "Column Label: stl4.25, Numerical Index: 453\n",
      "Column Label: fsr.1, Numerical Index: 454\n",
      "Column Label: fsr.2, Numerical Index: 455\n",
      "Column Label: fsr.3, Numerical Index: 456\n",
      "Column Label: fsr.4, Numerical Index: 457\n",
      "Column Label: fsr.5, Numerical Index: 458\n",
      "Column Label: fsr.6, Numerical Index: 459\n",
      "Column Label: fsr.7, Numerical Index: 460\n",
      "Column Label: fsr.8, Numerical Index: 461\n",
      "Column Label: fsr.9, Numerical Index: 462\n",
      "Column Label: fsr.10, Numerical Index: 463\n",
      "Column Label: fsr.11, Numerical Index: 464\n",
      "Column Label: fsr.12, Numerical Index: 465\n",
      "Column Label: fsr.13, Numerical Index: 466\n",
      "Column Label: fsr.14, Numerical Index: 467\n",
      "Column Label: fsr.15, Numerical Index: 468\n",
      "Column Label: fsr.16, Numerical Index: 469\n",
      "Column Label: fsr.17, Numerical Index: 470\n",
      "Column Label: fsr.18, Numerical Index: 471\n",
      "Column Label: fsr.19, Numerical Index: 472\n",
      "Column Label: fsr.20, Numerical Index: 473\n",
      "Column Label: fsr.21, Numerical Index: 474\n",
      "Column Label: fsr.22, Numerical Index: 475\n",
      "Column Label: fsr.23, Numerical Index: 476\n",
      "Column Label: fsr.24, Numerical Index: 477\n",
      "Column Label: fsr.25, Numerical Index: 478\n",
      "Column Label: flsr.1, Numerical Index: 479\n",
      "Column Label: flsr.2, Numerical Index: 480\n",
      "Column Label: flsr.3, Numerical Index: 481\n",
      "Column Label: flsr.4, Numerical Index: 482\n",
      "Column Label: flsr.5, Numerical Index: 483\n",
      "Column Label: flsr.6, Numerical Index: 484\n",
      "Column Label: flsr.7, Numerical Index: 485\n",
      "Column Label: flsr.8, Numerical Index: 486\n",
      "Column Label: flsr.9, Numerical Index: 487\n",
      "Column Label: flsr.10, Numerical Index: 488\n",
      "Column Label: flsr.11, Numerical Index: 489\n",
      "Column Label: flsr.12, Numerical Index: 490\n",
      "Column Label: flsr.13, Numerical Index: 491\n",
      "Column Label: flsr.14, Numerical Index: 492\n",
      "Column Label: flsr.15, Numerical Index: 493\n",
      "Column Label: flsr.16, Numerical Index: 494\n",
      "Column Label: flsr.17, Numerical Index: 495\n",
      "Column Label: flsr.18, Numerical Index: 496\n",
      "Column Label: flsr.19, Numerical Index: 497\n",
      "Column Label: flsr.20, Numerical Index: 498\n",
      "Column Label: flsr.21, Numerical Index: 499\n",
      "Column Label: flsr.22, Numerical Index: 500\n",
      "Column Label: flsr.23, Numerical Index: 501\n",
      "Column Label: flsr.24, Numerical Index: 502\n",
      "Column Label: flsr.25, Numerical Index: 503\n",
      "Column Label: u100.1, Numerical Index: 504\n",
      "Column Label: u100.2, Numerical Index: 505\n",
      "Column Label: u100.3, Numerical Index: 506\n",
      "Column Label: u100.4, Numerical Index: 507\n",
      "Column Label: u100.5, Numerical Index: 508\n",
      "Column Label: u100.6, Numerical Index: 509\n",
      "Column Label: u100.7, Numerical Index: 510\n",
      "Column Label: u100.8, Numerical Index: 511\n",
      "Column Label: u100.9, Numerical Index: 512\n",
      "Column Label: u100.10, Numerical Index: 513\n",
      "Column Label: u100.11, Numerical Index: 514\n",
      "Column Label: u100.12, Numerical Index: 515\n",
      "Column Label: u100.13, Numerical Index: 516\n",
      "Column Label: u100.14, Numerical Index: 517\n",
      "Column Label: u100.15, Numerical Index: 518\n",
      "Column Label: u100.16, Numerical Index: 519\n",
      "Column Label: u100.17, Numerical Index: 520\n",
      "Column Label: u100.18, Numerical Index: 521\n",
      "Column Label: u100.19, Numerical Index: 522\n",
      "Column Label: u100.20, Numerical Index: 523\n",
      "Column Label: u100.21, Numerical Index: 524\n",
      "Column Label: u100.22, Numerical Index: 525\n",
      "Column Label: u100.23, Numerical Index: 526\n",
      "Column Label: u100.24, Numerical Index: 527\n",
      "Column Label: u100.25, Numerical Index: 528\n",
      "Column Label: v100.1, Numerical Index: 529\n",
      "Column Label: v100.2, Numerical Index: 530\n",
      "Column Label: v100.3, Numerical Index: 531\n",
      "Column Label: v100.4, Numerical Index: 532\n",
      "Column Label: v100.5, Numerical Index: 533\n",
      "Column Label: v100.6, Numerical Index: 534\n",
      "Column Label: v100.7, Numerical Index: 535\n",
      "Column Label: v100.8, Numerical Index: 536\n",
      "Column Label: v100.9, Numerical Index: 537\n",
      "Column Label: v100.10, Numerical Index: 538\n",
      "Column Label: v100.11, Numerical Index: 539\n",
      "Column Label: v100.12, Numerical Index: 540\n",
      "Column Label: v100.13, Numerical Index: 541\n",
      "Column Label: v100.14, Numerical Index: 542\n",
      "Column Label: v100.15, Numerical Index: 543\n",
      "Column Label: v100.16, Numerical Index: 544\n",
      "Column Label: v100.17, Numerical Index: 545\n",
      "Column Label: v100.18, Numerical Index: 546\n",
      "Column Label: v100.19, Numerical Index: 547\n",
      "Column Label: v100.20, Numerical Index: 548\n",
      "Column Label: v100.21, Numerical Index: 549\n",
      "Column Label: v100.22, Numerical Index: 550\n",
      "Column Label: v100.23, Numerical Index: 551\n",
      "Column Label: v100.24, Numerical Index: 552\n",
      "Column Label: v100.25, Numerical Index: 553\n",
      "Column Label: wind_speed1, Numerical Index: 554\n",
      "Column Label: wind_speed2, Numerical Index: 555\n",
      "Column Label: wind_speed3, Numerical Index: 556\n",
      "Column Label: wind_speed4, Numerical Index: 557\n",
      "Column Label: wind_speed5, Numerical Index: 558\n",
      "Column Label: wind_speed6, Numerical Index: 559\n",
      "Column Label: wind_speed7, Numerical Index: 560\n",
      "Column Label: wind_speed8, Numerical Index: 561\n",
      "Column Label: wind_speed9, Numerical Index: 562\n",
      "Column Label: wind_speed10, Numerical Index: 563\n",
      "Column Label: wind_speed11, Numerical Index: 564\n",
      "Column Label: wind_speed12, Numerical Index: 565\n",
      "Column Label: wind_speed13, Numerical Index: 566\n",
      "Column Label: wind_speed14, Numerical Index: 567\n",
      "Column Label: wind_speed15, Numerical Index: 568\n",
      "Column Label: wind_speed16, Numerical Index: 569\n",
      "Column Label: wind_speed17, Numerical Index: 570\n",
      "Column Label: wind_speed18, Numerical Index: 571\n",
      "Column Label: wind_speed19, Numerical Index: 572\n",
      "Column Label: wind_speed20, Numerical Index: 573\n",
      "Column Label: wind_speed21, Numerical Index: 574\n",
      "Column Label: wind_speed22, Numerical Index: 575\n",
      "Column Label: wind_speed23, Numerical Index: 576\n",
      "Column Label: wind_speed24, Numerical Index: 577\n",
      "Column Label: wind_speed25, Numerical Index: 578\n"
     ]
    }
   ],
   "source": [
    "column_indices = X.columns\n",
    "numerical_indices = range(len(X.columns))\n",
    "for numerical_index, column_label in enumerate(X.columns):\n",
    "    print(f\"Column Label: {column_label}, Numerical Index: {numerical_index}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d688035-6429-491e-9bad-7b669ee1f5e3",
   "metadata": {},
   "source": [
    "explicar aqui que filtramos el dataset original con las features que nos ha dado la pipeline. Destacas que las seleccionadas son: \n",
    "- p59.162: Vertical integral of divergence of kinetic energy\n",
    "- u10n: Neutral wind at 10 m u-component\n",
    "- v10n: Neutral wind at 10 m v-component\n",
    "- u10: 10 metre U wind component\n",
    "-• v10: 10 metre V wind componen\n",
    "- iews: Instantaneous eastward turbulent surface stress\n",
    "- inss: Instantaneous northward turbulent surface\n",
    "- flsr: Forecast logarithm of surface roughness for heat\n",
    "- u100: 100 metre U wind component\n",
    "-  v100: 100 metre V wind componen\n",
    "- wind_speed:  modulus of u100 and v100tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "428aefc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['p59.162.1', 'p59.162.2', 'p59.162.3', 'p59.162.4', 'p59.162.5',\n",
       "       'p59.162.6', 'p59.162.7', 'p59.162.9', 'p59.162.11', 'p59.162.12',\n",
       "       ...\n",
       "       'wind_speed16', 'wind_speed17', 'wind_speed18', 'wind_speed19',\n",
       "       'wind_speed20', 'wind_speed21', 'wind_speed22', 'wind_speed23',\n",
       "       'wind_speed24', 'wind_speed25'],\n",
       "      dtype='object', length=250)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only the selected features in your original data X\n",
    "X_selected_features = X.iloc[:, selected_feature_indices]\n",
    "X_test = X_test.iloc[:, selected_feature_indices]\n",
    "X_selected_features.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864a71ae-a68f-431a-a595-9ec0cc508635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "772ca61a",
   "metadata": {},
   "source": [
    "### 3.2. Imputation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7081fa1-401b-41f2-9512-307a79aceefb",
   "metadata": {},
   "source": [
    "Does imputation improve performance in this problem? Which method seem to work best?\n",
    "\n",
    "Intentamos primero elegir tipos de simple imputer, y luego ver si el iterative baja el MAE más."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "931d07ff-220d-4df9-aee7-63428beeda6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model MAE: 320.0383530482257\n",
      "Selected Imputation Strategy: median\n"
     ]
    }
   ],
   "source": [
    "# Load a sample dataset for demonstration\n",
    "X = X_selected_features\n",
    "y = y_train\n",
    "\n",
    "# Define the holdout split\n",
    "indices = ([-1] * len(X_train_train)) + ([0] * len(X_train_validation))\n",
    "inner = PredefinedSplit(indices)\n",
    "\n",
    "# Create a new pipeline with imputation after feature selection\n",
    "pipeline_simple_imputer = Pipeline([\n",
    "    ('imputer', SimpleImputer()), \n",
    "    ('scaler', StandardScaler()),  # Placeholder, will be replaced during grid search\n",
    "    ('knn', KNeighborsRegressor(n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Define the hyperparameter grid for GridSearchCV\n",
    "param_grid_si = {\n",
    "    'imputer__strategy': ['mean', 'median', 'most_frequent']\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to find the best combination of parameters\n",
    "grid_search = RandomizedSearchCV(pipeline_simple_imputer, param_grid_si, cv=inner, scoring='neg_mean_absolute_error', n_jobs=-1, random_state=random_state)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate and print the Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Best Model MAE: {mae}\")\n",
    "# Print the selected imputation strategy\n",
    "selected_imputer_strategy = best_model.named_steps['imputer'].strategy\n",
    "print(f\"Selected Imputation Strategy: {selected_imputer_strategy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca95b53-951c-49e6-859e-a26e0fcc8f81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load a sample dataset for demonstration\n",
    "X = X_selected_features\n",
    "y = y_train\n",
    "\n",
    "# Define the holdout split\n",
    "indices = ([-1] * len(X_train_train)) + ([0] * len(X_train_validation))\n",
    "inner = PredefinedSplit(indices)\n",
    "\n",
    "# Create a new pipeline with imputation after feature selection\n",
    "pipeline_iterative_imputer = Pipeline([\n",
    "    ('imputer', IterativeImputer()), \n",
    "    ('scaler', StandardScaler()),  # Placeholder, will be replaced during grid search\n",
    "    ('knn', KNeighborsRegressor(n_jobs=-1))\n",
    "])\n",
    "\n",
    "pipeline_iterative_imputer.fit(X, y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = pipeline_iterative_imputer.predict(X_test)\n",
    "\n",
    "# Calculate and print the Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Iterative imputer MAE: {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46c0ea-0dcb-428a-8a73-61337a2a2d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da46695f-d49a-4be9-b61b-623b48da0f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample dataset for demonstration\n",
    "X = X_selected_features\n",
    "y = y_train\n",
    "\n",
    "# Define the holdout split\n",
    "indices = ([-1] * len(X_train_train)) + ([0] * len(X_train_validation))\n",
    "inner = PredefinedSplit(indices)\n",
    "\n",
    "# Create a new pipeline with imputation after feature selection\n",
    "pipeline_KNN_imputer = Pipeline([\n",
    "    ('imputer', KNNImputer()), \n",
    "    ('scaler', StandardScaler()),  \n",
    "    ('knn', KNeighborsRegressor(n_jobs=-1))\n",
    "])\n",
    "\n",
    "pipeline_KNN_imputer.fit(X, y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = pipeline_KNN_imputer.predict(X_test)\n",
    "\n",
    "# Calculate and print the Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"KNN imputer MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a98b43-5761-4339-bb23-bbd5b5dbe89e",
   "metadata": {},
   "source": [
    "Como el MAE es menor para el iterative, usamos este para transformar nuestros datos y continuar al último paso del preprocesado, el scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6ed6e8-d7dc-40b7-b59e-221868bf9af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the imputed data\n",
    "X_imputed = pipeline_iterative_imputer.named_steps['imputer'].transform(X)\n",
    "X_imputed \n",
    "X_test = pipeline_iterative_imputer.named_steps['imputer'].transform(X_test)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6864c55e",
   "metadata": {},
   "source": [
    "### 3.3. Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0e85c8-1bc4-4a5b-bfd1-d76c1ea30805",
   "metadata": {},
   "source": [
    "Por último, ahora que ya tenemos las mejores features seleccionadas y habiendo imputado con el iterative, hay que ver qué método de escalado es mejor (esto luego solo afecta a los métodos que calculan distancias como el knn pero se aplicará al dataset para todos los casos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1411155a-f1d3-4acc-8e06-0408e3bf3706",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_imputed\n",
    "y = y_train\n",
    "\n",
    "# Define the holdout split\n",
    "indices = ([-1] * len(X_train_train)) + ([0] * len(X_train_validation))\n",
    "inner = PredefinedSplit(indices)\n",
    "\n",
    "# Create a new pipeline with scaling after feature selection and imputation\n",
    "pipeline_standard_scaler = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('knn', KNeighborsRegressor(n_jobs=-1))\n",
    "])\n",
    "\n",
    "pipeline_standard_scaler.fit(X, y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = pipeline_standard_scaler.predict(X_test)\n",
    "\n",
    "# Calculate and print the Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Standard scaler MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e619e178-1500-4aab-95b9-6566844f361e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new pipeline with scaling after feature selection and imputation\n",
    "pipeline_minmax_scaler = Pipeline([\n",
    "    ('scaler', MinMaxScaler()), \n",
    "    ('knn', KNeighborsRegressor(n_jobs=-1))\n",
    "])\n",
    "\n",
    "pipeline_minmax_scaler.fit(X, y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = pipeline_minmax_scaler.predict(X_test)\n",
    "\n",
    "# Calculate and print the Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"MinMax scaler MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5b926f-b0f8-4281-aad7-61fafa4a108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new pipeline with scaling after feature selection and imputation\n",
    "pipeline_robust_scaler = Pipeline([\n",
    "    ('scaler', RobustScaler()), \n",
    "    ('knn', KNeighborsRegressor(n_jobs=-1))\n",
    "])\n",
    "\n",
    "pipeline_robust_scaler.fit(X, y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = pipeline_robust_scaler.predict(X_test)\n",
    "\n",
    "# Calculate and print the Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Robust scaler MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc41d214-1a0f-48ec-8412-449e4b91b3cd",
   "metadata": {},
   "source": [
    "### 3.4. Final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d647db-cd51-45ef-adf6-4c23e81ef517",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pipeline_robust_scaler.named_steps['scaler'].transform(X)\n",
    "X_train\n",
    "X_test = pipeline_robust_scaler.named_steps['scaler'].transform(X_test)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "999f7f1f-98d3-4398-a083-8284a60f32b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train and X_test are NumPy arrays\n",
    "#np.savetxt('X_train.csv', X_train, delimiter=';')\n",
    "#np.savetxt('X_test.csv', X_test, delimiter=';')\n",
    "#np.savetxt('X_train_train.csv', X_train_train, delimiter=';')\n",
    "#np.savetxt('X_train_validation.csv', X_train_validation, delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d49ed8a-0951-4c12-8074-f704c5d2178b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.31177367, -0.31696642, -0.31655972, ..., -0.23377745,\n",
       "        -0.21241713, -0.19716302],\n",
       "       [-0.09378879, -0.10216953, -0.10638298, ...,  0.11043825,\n",
       "         0.10968315,  0.1042859 ],\n",
       "       [ 0.05649893,  0.04862603,  0.04390409, ...,  0.70883047,\n",
       "         0.67642773,  0.64116408],\n",
       "       ...,\n",
       "       [ 0.54200045,  0.58215538,  0.62177319, ..., -0.2946182 ,\n",
       "        -0.25202736, -0.21299333],\n",
       "       [ 0.09334912,  0.09679973,  0.10300574, ...,  0.88391007,\n",
       "         0.86213978,  0.84281453],\n",
       "       [ 0.06947422,  0.07564338,  0.08476866, ...,  0.47437584,\n",
       "         0.52065433,  0.57001822]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.loadtxt('X_train.csv', delimiter=';')\n",
    "X_test = np.loadtxt('X_test.csv', delimiter=';')\n",
    "X_train_train = np.loadtxt('X_train_train.csv', delimiter=';')\n",
    "X_train_validation = np.loadtxt('X_train_validation.csv', delimiter=';')\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787a8dc9-79fd-4fcf-9665-a3e7bdf02848",
   "metadata": {},
   "source": [
    "## 4. Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a96cbd-719c-4c41-93b1-ec42b379972a",
   "metadata": {},
   "source": [
    "### 4.1. Dummy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03985d5-0a23-4ec8-94e4-6008013af6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store results\n",
    "results_df = pd.DataFrame(columns=['Model', 'MAE', 'Training Time'])\n",
    "\n",
    "# Function to add results to the DataFrame\n",
    "def add_result(model_name, mae, training_time):\n",
    "    results_df.loc[len(results_df)] = [model_name, mae, training_time]\n",
    "\n",
    "indices = ([-1] * len(X_train_train)) + ([0] * len(X_train_validation))\n",
    "inner = PredefinedSplit(indices)\n",
    "\n",
    "dummy_reg =DummyRegressor(strategy='median')\n",
    "\n",
    "# Measure training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the model and calculate scores\n",
    "dummy_scores = cross_val_score(dummy_reg, X_train, y_train, cv=inner, scoring='neg_mean_absolute_error')\n",
    "\n",
    "# Measure end time\n",
    "end_time = time.time()\n",
    "\n",
    "training_time= end_time-start_time\n",
    "\n",
    "# Add results to the DataFrame\n",
    "add_result('Dummy Mean', -dummy_scores.mean(), training_time)\n",
    "\n",
    "# Print the results DataFrame\n",
    "results_df\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774d1b5d-0cd2-4ed4-a915-eebc7075ae92",
   "metadata": {},
   "source": [
    "### 4.2. Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9857b2-6ef6-4d19-a6f6-5bc5ce7fad06",
   "metadata": {},
   "source": [
    "#### 4.2.1 Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4599381-35d4-4b3b-bf13-a10009059926",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg = DecisionTreeRegressor(random_state=random_state)\n",
    "\n",
    "# Measure training time\n",
    "start_time = time.time()\n",
    "\n",
    "tree_default_scores = cross_val_score(tree_reg,  X_train, y_train, cv=inner, scoring='neg_mean_absolute_error')\n",
    "\n",
    "# Measure end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate training time\n",
    "training_time = end_time - start_time\n",
    "\n",
    "add_result('Tree Default', -tree_default_scores.mean(), training_time)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981c1092-3c74-4e20-affb-fbe8daed6bc2",
   "metadata": {},
   "source": [
    "#### 4.2.2. HPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d499a1e1-59da-492f-8c08-a76b6c3e5301",
   "metadata": {},
   "source": [
    "Decision trees hyperparameters:\n",
    "- max_depth: the maximum depth of the tree\n",
    "- \tmin_samples_split: the minimum number of instances to splits a node (the default value is 2: if a node contains fewer  than instances, the node is not split)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e59c22c-bced-454c-b1f1-f18e71ad5abd",
   "metadata": {},
   "source": [
    "##### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe039ba-0c86-451b-8e0c-74c36590a67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = ([-1] * len(X_train_train)) + ([0] * len(X_train_validation))\n",
    "inner = PredefinedSplit(indices)\n",
    "\n",
    "param_random_tree = {'max_depth': [2,4,6,8,10,12,14,16,18,20],\n",
    "                     'min_samples_split': [100,110,120,130]}\n",
    "\n",
    "random_search_tree = RandomizedSearchCV(tree_reg, param_random_tree, cv= inner, scoring='neg_mean_absolute_error', random_state= random_state) \n",
    "\n",
    "# Measure training time\n",
    "start_time = time.time()\n",
    "\n",
    "random_search_tree.fit(X_train, y_train)\n",
    "\n",
    "# Measure end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate training time\n",
    "training_time = end_time - start_time\n",
    "\n",
    "#We print the selected parameters to be sure we are not on the boundaries of the search space\n",
    "\n",
    "random_search_tree.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe9d459-2a92-44be-bbd4-1f219bb19590",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_result('Tree Random HPO', -random_search_tree.best_score_, training_time)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ffa191-0640-4c94-8fe3-2cc5dafe43fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39b72f6a-617f-42bc-9e89-f4723822ca4f",
   "metadata": {},
   "source": [
    "##### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8151ea8f-43c6-43fd-b12e-15600af82b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = ([-1] * len(X_train_train)) + ([0] * len(X_train_validation))\n",
    "inner = PredefinedSplit(indices)\n",
    "\n",
    "param_grid_tree = {'max_depth': [2,4,6,8,10,12,14,16,18,20],\n",
    "                     'min_samples_split': [100,110,120,130]}\n",
    "\n",
    "grid_search_tree = GridSearchCV(tree_reg, param_grid_tree, cv= inner, scoring='neg_mean_absolute_error') \n",
    "\n",
    "# Measure training time\n",
    "start_time = time.time()\n",
    "\n",
    "grid_search_tree.fit(X_train, y_train)\n",
    "\n",
    "# Measure end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate training time\n",
    "training_time = end_time - start_time\n",
    "\n",
    "add_result('Tree Grid HPO', -grid_search_tree.best_score_, training_time)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e867b0-2968-4633-ac26-b9482ed66c29",
   "metadata": {},
   "source": [
    "##### Bayes optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3e2d8a-4c1b-4521-98d8-1c92c428eaec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from skopt.space import Integer\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "indices = ([-1] * len(X_train_train)) + ([0] * len(X_train_validation))\n",
    "inner = PredefinedSplit(indices)\n",
    "\n",
    "param_bayes_tree = {'max_depth': Integer(2, 20),\n",
    "                     'min_samples_split': Integer(100, 130)}\n",
    "\n",
    "budget=30 #number of iterations\n",
    "bayes_search_tree = BayesSearchCV(tree_reg, \n",
    "                                 param_bayes_tree, \n",
    "                                 cv= inner, \n",
    "                                 scoring='neg_mean_absolute_error',\n",
    "                                n_iter=budget,\n",
    "                                n_jobs=-1, verbose=1) \n",
    "\n",
    "\n",
    "# Measure training time\n",
    "start_time = time.time()\n",
    "\n",
    "bayes_search_tree.fit(X_train, y_train)\n",
    "\n",
    "# Measure end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate training time\n",
    "training_time = end_time - start_time\n",
    "\n",
    "add_result('Tree Bayes HPO', -bayes_search_tree.best_score_, training_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0f8736-1f27-42fc-8b0a-9cc7b1062f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6847a9a9-e9af-4b23-8b30-64566fd5a004",
   "metadata": {},
   "source": [
    "##### Halving Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5bc9b2-ad1d-4f6e-b343-ecda987e058c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "indices = ([-1] * len(X_train_train)) + ([0] * len(X_train_validation))\n",
    "inner = PredefinedSplit(indices)\n",
    "\n",
    "param_halving_tree = {'max_depth': list(range(2, 20, 1)),\n",
    "                     'min_samples_split':list(range(100, 130, 1))}\n",
    "\n",
    "halving_search_tree = HalvingGridSearchCV(tree_reg, \n",
    "                                 param_halving_tree, \n",
    "                                 cv= inner, \n",
    "                                 scoring='neg_mean_absolute_error',\n",
    "                                factor= 2, #proportion of candidates that are selected in each iteration\n",
    "                                min_resources='exhaust', #“exhaust” means that first iteration is as large as possible, so that in the last iteration, the entire training data is used.\n",
    "                                max_resources='auto',\n",
    "                                n_jobs=-1, verbose=1) \n",
    "\n",
    "\n",
    "# Measure training time\n",
    "start_time = time.time()\n",
    "\n",
    "halving_search_tree.fit(X_train, y_train)\n",
    "\n",
    "# Measure end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate training time\n",
    "training_time = end_time - start_time\n",
    "\n",
    "add_result('Tree Halving HPO', -halving_search_tree.best_score_, training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4816190e-4ba0-4709-bc2e-666a39b57eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa090d25-4ca7-4374-957e-25cf698aeddc",
   "metadata": {},
   "source": [
    "### 4.3. KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77317e4e-b992-426e-b45e-5b2f682731a4",
   "metadata": {},
   "source": [
    "#### 4.3.1. Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d4029-f1c1-44cc-a100-2eda9fcdcd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = ([-1] * len(X_train_train)) + ([0] * len(X_train_validation))\n",
    "inner = PredefinedSplit(indices)\n",
    "\n",
    "# Create a KNN regressor with default parameters\n",
    "knn_reg = KNeighborsRegressor(n_jobs=-1)\n",
    "\n",
    "# Measure training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform cross-validation and calculate mean absolute error\n",
    "knn_scores = cross_val_score(knn_reg, X_train, y_train, cv=inner, scoring='neg_mean_absolute_error')\n",
    "\n",
    "# Measure end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate training time\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Add the results to a dataframe or any data structure you're using to store results\n",
    "add_result('KNN Default', -knn_scores.mean(), training_time)\n",
    "\n",
    "# Assuming 'add_result' is a function that adds results to your 'results_df'\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cd906f-b2c4-4a65-99f6-cdd2f881ea41",
   "metadata": {},
   "source": [
    "#### 4.3.2. HPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745cd79b-d12b-4bf1-8efd-8fa2ff5ebebd",
   "metadata": {},
   "source": [
    "##### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad83205-a7e2-44a0-b954-05f2e538af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for random search\n",
    "param_dist = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],  \n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "}\n",
    "\n",
    "# Create a KNN regressor with default parameters\n",
    "knn_reg = KNeighborsRegressor()\n",
    "\n",
    "# Define the randomized search object\n",
    "random_search = RandomizedSearchCV(knn_reg, param_distributions=param_dist, scoring='neg_mean_absolute_error', cv=inner, n_jobs=-1,random_state= random_state)\n",
    "\n",
    "# Measure training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the randomized search to the data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Measure end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate training time\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Print the mean absolute error on the validation set\n",
    "print(\"Validation MAE:\", -random_search.best_score_)\n",
    "\n",
    "# Print training time\n",
    "print(\"Training Time:\", training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eafbd1-af43-4d33-8f8d-b8e0685a8ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_result('KNN Random HPO', -random_search.best_score_, training_time)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a95ad58-0729-4e52-8205-c4b25cb2bf85",
   "metadata": {},
   "source": [
    "##### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b242de-a3bd-45b2-bd90-8b88fbf7038e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],  \n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "}\n",
    "\n",
    "# Create a KNN regressor with default parameters\n",
    "knn_reg = KNeighborsRegressor()\n",
    "\n",
    "# Define the grid search object\n",
    "grid_search_knn = GridSearchCV(knn_reg, param_grid=param_grid, scoring='neg_mean_absolute_error', cv=inner, n_jobs=-1)\n",
    "\n",
    "# Measure training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search_knn.fit(X_train, y_train)\n",
    "\n",
    "# Measure end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate training time\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search_knn.best_params_)\n",
    "\n",
    "# Print the mean absolute error on the validation set\n",
    "print(\"Validation MAE:\", -grid_search_knn.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7094f6-3818-4f01-a275-6f5b6acf64e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the results to the dataframe\n",
    "add_result('KNN Grid HPO', -grid_search_knn.best_score_, training_time)\n",
    "\n",
    "# Display the results dataframe\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6b149e-048b-4280-b3a1-46a0831477a3",
   "metadata": {},
   "source": [
    "##### Bayes optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30737d32-2f07-4b58-8122-a26574bf3a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter search space for Bayesian optimization\n",
    "param_space = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11], \n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "}\n",
    "\n",
    "# Create a KNN regressor with default parameters\n",
    "knn_reg = KNeighborsRegressor()\n",
    "\n",
    "budget=20\n",
    "# Define the Bayesian search object\n",
    "bayes_search_knn = BayesSearchCV(knn_reg, search_spaces=param_space, scoring='neg_mean_absolute_error', cv=inner, n_jobs=-1, n_iter=budget, verbose=1)\n",
    "\n",
    "# Measure training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the Bayesian search to the data\n",
    "bayes_search_knn.fit(X_train, y_train)\n",
    "\n",
    "# Measure end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate training time\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", bayes_search_knn.best_params_)\n",
    "\n",
    "# Print the mean absolute error on the validation set\n",
    "print(\"Validation MAE:\", -bayes_search_knn.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ac2f05-549d-4743-bed4-3515bb179f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the results to the dataframe\n",
    "add_result('KNN Bayes HPO', -bayes_search_knn.best_score_, training_time)\n",
    "\n",
    "# Display the results dataframe\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727ae5cd-bac8-4a32-b7b3-39df313093cf",
   "metadata": {},
   "source": [
    "##### Halving Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7fc561-486e-498d-985f-8d6b0470caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KNeighborsRegressor instance\n",
    "knn_reg = KNeighborsRegressor()\n",
    "\n",
    "# Create a PredefinedSplit to split the data into training and validation sets\n",
    "indices = ([-1] * len(X_train_train)) + ([0] * len(X_train_validation))\n",
    "inner = PredefinedSplit(indices)\n",
    "\n",
    "# Define the parameter search space for the Halving Grid Search\n",
    "param_halving_knn = {\n",
    "    'n_neighbors': list(range(3, 12, 2)),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "}\n",
    "\n",
    "# Create a HalvingGridSearchCV instance for KNN\n",
    "halving_search_knn = HalvingGridSearchCV(\n",
    "    knn_reg,\n",
    "    param_halving_knn,\n",
    "    cv=inner,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    factor=2,  # proportion of candidates that are selected in each iteration\n",
    "    min_resources='exhaust',  # “exhaust” means that the first iteration is as large as possible\n",
    "    max_resources='auto',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Measure training time\n",
    "start_time = time.time()\n",
    "\n",
    "halving_search_knn.fit(X_train, y_train)\n",
    "\n",
    "# Measure end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate training time\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", halving_search_knn.best_params_)\n",
    "print(\"Best Score:\", -halving_search_knn.best_score_)\n",
    "print(\"Training Time:\", training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88518cfb-75cb-4e72-a221-da1f6eb4590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_result('KNN Halving HPO', -halving_search_knn.best_score_, training_time)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c5a5d-97b9-4228-94d5-d0d6d1adc140",
   "metadata": {},
   "source": [
    "### 4.4. Ensemble 1: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6651bd23-c491-4f41-802a-12b24088c1a3",
   "metadata": {},
   "source": [
    "#### 4.4.1. Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b34dbb3-704c-40b9-b85e-873a2c242e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_for_reg = RandomForestRegressor(random_state=random_state)\n",
    "\n",
    "# Measure training time\n",
    "start_time = time.time()\n",
    "\n",
    "rand_for_default_scores = cross_val_score(rand_for_reg,  X_train, y_train, cv=inner, scoring='neg_mean_absolute_error')\n",
    "\n",
    "# Measure end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate training time\n",
    "training_time = end_time - start_time\n",
    "\n",
    "add_result('RF Default', -rand_for_default_scores.mean(), training_time)\n",
    "\n",
    "results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec6c48f-661c-4a06-8aa4-65e50913c4c6",
   "metadata": {},
   "source": [
    "#### 4.4.2. HPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23da3aa7-6897-46a2-9be7-c418969ff306",
   "metadata": {},
   "source": [
    "Random forest Hyperparameters:\n",
    "- n_estimators: number of trees (base models) in the\n",
    "ensemble: there should be enough trees in the\n",
    "ensemble. The default (100) usually works well, but it\n",
    "can also be tuned.\n",
    "- max_features : mtry or m. Size of the random subset\n",
    "of features to be compared at every node. Possible\n",
    "values: sqrt, log2, or a number between 1 and the\n",
    "maximum number of features in the dataset. It can also be a real number between 0 and 1 (fraction of\n",
    "features)\n",
    "- max_depth / min_samples_split: hyper parameters of\n",
    "the trees (base models\n",
    "- njobs is not a hyper parameter, but allows the RF to\n",
    "be trained in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c975b1-ab68-45b4-8e28-fdb6ffa8c783",
   "metadata": {},
   "source": [
    "##### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7962fc22-7793-4864-bc30-0233811c5e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = ([-1] * len(X_train_train)) + ([0] * len(X_train_validation))\n",
    "inner = PredefinedSplit(indices)\n",
    "\n",
    "param_random_RF = {'max_depth': [2,4,6,8,10,12,14,16,18,20],\n",
    "                     'min_samples_split': [100,110,120,130],\n",
    "                  'n_estimators':[25,50,100,150],\n",
    "                  'max_features':[0.2,0.4,0.6,0.8,1]}\n",
    "\n",
    "random_search_RF = RandomizedSearchCV(rand_for_reg, param_random_RF, cv= inner, scoring='neg_mean_absolute_error', random_state= random_state) \n",
    "\n",
    "# Measure training time\n",
    "start_time = time.time()\n",
    "\n",
    "random_search_RF.fit(X_train, y_train)\n",
    "\n",
    "# Measure end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate training time\n",
    "training_time = end_time - start_time\n",
    "\n",
    "#We print the selected parameters to be sure we are not on the boundaries of the search space\n",
    "\n",
    "random_search_RF.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b0b719-6969-4626-935a-a1b668ccf1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_result('RF Random HPO', -random_search_RF.best_score_, training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e32b4e1-db6d-4656-86c0-962e157e0343",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de8ba2a-f712-474d-8b6e-673dc72f75de",
   "metadata": {},
   "source": [
    "##### Grid Search and Bayesian Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbf3368-658c-4877-9c26-2dfa361e053c",
   "metadata": {},
   "source": [
    "As we have already observed with the HPO of the trees, Grid Search and Bayesian Search takes too much time compared to the minimization of the MAE. Therefore, as RF is a ensemble model based on trees, we may expect the same behaviour, but even with a higher increase (as already the random search has increased the computing time a lot). Therefore, we would not try to do HPO neither with Grid or Bayesian Search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73901a1f-9233-4b48-b2b3-2525214ce2f7",
   "metadata": {},
   "source": [
    "##### Halving Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227e6f03-b61e-4a5e-9e02-6059fd60bc4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indices = ([-1] * len(X_train_train)) + ([0] * len(X_train_validation))\n",
    "inner = PredefinedSplit(indices)\n",
    "\n",
    "param_halving_RF = {'max_depth': list(range(2, 20, 2)),\n",
    "                     'min_samples_split':list(range(100, 130, 5)),\n",
    "                     'n_estimators':list(range(50,150, 50)),\n",
    "                       'max_features':[0.2,0.4,0.6,0.8,1]}\n",
    "\n",
    "\n",
    "halving_search_RF = HalvingGridSearchCV(rand_for_reg, \n",
    "                                 param_halving_RF, \n",
    "                                 cv= inner, \n",
    "                                 scoring='neg_mean_absolute_error',\n",
    "                                factor= 2, #proportion of candidates that are selected in each iteration\n",
    "                                min_resources='exhaust', #“exhaust” means that first iteration is as large as possible, so that in the last iteration, the entire training data is used.\n",
    "                                max_resources='auto',\n",
    "                                n_jobs=-1, verbose=1) \n",
    "\n",
    "\n",
    "# Measure training time\n",
    "start_time = time.time()\n",
    "\n",
    "halving_search_RF.fit(X_train, y_train)\n",
    "\n",
    "# Measure end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate training time\n",
    "training_time = end_time - start_time\n",
    "\n",
    "\n",
    "halving_search_RF.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d95c7fb-e418-428f-8233-4f9f69ebfafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_result('RF Halving HPO', -halving_search_RF.best_score_, training_time)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3366bb8-4f5a-4f35-aded-ea907cf7e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3de220-c92b-430d-9b5c-7d57aca57219",
   "metadata": {},
   "source": [
    "### 4.5. Ensemble 2: Gradient Boosting\n",
    "custom loss function for boosting!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbec1058-7402-4a50-8f08-f022da9ab735",
   "metadata": {},
   "source": [
    "#### 4.5.1. Default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f434820-d792-4b84-8695-94ce0ae3ac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Create a GradientBoostingRegressor instance with default parameters\n",
    "gb_reg = GradientBoostingRegressor(random_state=random_state)\n",
    "\n",
    "# Measure training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Use cross_val_score for cross-validation\n",
    "gb_default_scores = cross_val_score(gb_reg, X_train, y_train, cv=inner, scoring='neg_mean_absolute_error')\n",
    "\n",
    "# Measure end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate training time\n",
    "training_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ef6181-6bf2-44ba-a721-8d424372549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the results to your results dataframe or store them as needed\n",
    "add_result('GB Default', -gb_default_scores.mean(), training_time)\n",
    "\n",
    "# Display or save your results dataframe\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929bdb8f-b4bc-4dbc-904d-7c1a5cb4e89f",
   "metadata": {},
   "source": [
    "#### 4.5.2. HPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2027fe66-f5cc-4986-97c2-45dc2d5ece74",
   "metadata": {},
   "source": [
    "##### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d2beb6-0aaf-4188-8c31-d69c40d7b0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GradientBoostingRegressor instance\n",
    "gb_reg = GradientBoostingRegressor()\n",
    "\n",
    "# Create a PredefinedSplit to split the data into training and validation sets\n",
    "indices = ([-1] * len(X_train_train)) + ([0] * len(X_train_validation))\n",
    "inner = PredefinedSplit(indices)\n",
    "\n",
    "# Define the parameter search space for RandomizedSearchCV\n",
    "param_random_gb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Create a RandomizedSearchCV instance for Gradient Boosting\n",
    "random_search_gb = RandomizedSearchCV(\n",
    "    gb_reg,\n",
    "    param_distributions=param_random_gb,\n",
    "    cv=inner,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=random_state  # Set a random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Measure training time\n",
    "start_time = time.time()\n",
    "\n",
    "random_search_gb.fit(X_train, y_train)\n",
    "\n",
    "# Measure end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate training time\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", random_search_gb.best_params_)\n",
    "print(\"Best Score:\", -random_search_gb.best_score_)\n",
    "print(\"Training Time:\", training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e070b98-d037-4091-be5c-92574d5d971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the results to your results dataframe or store them as needed\n",
    "add_result('GB Random HPO', -random_search_gb.best_score_, training_time)\n",
    "\n",
    "# Display or save your results dataframe\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2810b53-e9a8-46bc-ab33-99788d117fb4",
   "metadata": {},
   "source": [
    "##### Halving Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e23c42b-152a-4b1b-9804-1f18e4b6dc5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 7\n",
      "n_required_iterations: 7\n",
      "n_possible_iterations: 7\n",
      "min_resources_: 57\n",
      "max_resources_: 3649\n",
      "aggressive_elimination: False\n",
      "factor: 2\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 108\n",
      "n_resources: 57\n",
      "Fitting 1 folds for each of 108 candidates, totalling 108 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 54\n",
      "n_resources: 114\n",
      "Fitting 1 folds for each of 54 candidates, totalling 54 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 27\n",
      "n_resources: 228\n",
      "Fitting 1 folds for each of 27 candidates, totalling 27 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 14\n",
      "n_resources: 456\n",
      "Fitting 1 folds for each of 14 candidates, totalling 14 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 7\n",
      "n_resources: 912\n",
      "Fitting 1 folds for each of 7 candidates, totalling 7 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 4\n",
      "n_resources: 1824\n",
      "Fitting 1 folds for each of 4 candidates, totalling 4 fits\n",
      "----------\n",
      "iter: 6\n",
      "n_candidates: 2\n",
      "n_resources: 3648\n",
      "Fitting 1 folds for each of 2 candidates, totalling 2 fits\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 150}\n",
      "Best Score: 280.119193288301\n",
      "Training Time: 131.08297872543335\n"
     ]
    }
   ],
   "source": [
    "# Create a GradientBoostingRegressor instance\n",
    "gb_reg = GradientBoostingRegressor()\n",
    "\n",
    "# Create a PredefinedSplit to split the data into training and validation sets\n",
    "indices = ([-1] * len(X_train_train)) + ([0] * len(X_train_validation))\n",
    "inner = PredefinedSplit(indices)\n",
    "\n",
    "# Define the parameter search space for the Halving Grid Search\n",
    "param_halving_gb = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Create a HalvingGridSearchCV instance for Gradient Boosting\n",
    "halving_search_gb = HalvingGridSearchCV(\n",
    "    gb_reg,\n",
    "    param_halving_gb,\n",
    "    cv=inner,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    factor=2,  # proportion of candidates that are selected in each iteration\n",
    "    min_resources='exhaust',  # “exhaust” means that the first iteration is as large as possible\n",
    "    max_resources='auto',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Measure training time\n",
    "start_time = time.time()\n",
    "\n",
    "halving_search_gb.fit(X_train, y_train)\n",
    "\n",
    "# Measure end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate training time\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", halving_search_gb.best_params_)\n",
    "print(\"Best Score:\", -halving_search_gb.best_score_)\n",
    "print(\"Training Time:\", training_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "311ed965-114a-40d4-9eee-2b2e3fe7a77a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'joblib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dump, load\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Save the model to a file\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mjoblib\u001b[49m\u001b[38;5;241m.\u001b[39mdump(halving_search_gb, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhalving_search_gb_model.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load the model from the file\u001b[39;00m\n\u001b[1;32m      6\u001b[0m halving_search_gb \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhalving_search_gb_model.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'joblib' is not defined"
     ]
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "# Save the model to a file\n",
    "joblib.dump(halving_search_gb, 'halving_search_gb_model.joblib')\n",
    "\n",
    "# Load the model from the file\n",
    "halving_search_gb = joblib.load('halving_search_gb_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12ae905-0792-444f-ba88-8802a7e43a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the results to your results dataframe or store them as needed\n",
    "add_result('GB Halving HPO', -halving_search_gb.best_score_, training_time)\n",
    "\n",
    "# Display or save your results dataframe\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c67b20-02e3-448e-b024-94b5ce1e1130",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_mae_index = results_df['MAE'].idxmin()\n",
    "results_df.loc[min_mae_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71458389-448c-42d5-9cef-1be7aa8364ed",
   "metadata": {},
   "source": [
    "### 4.6. Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aaec82-859a-4f0f-9d6f-77d57ad133dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "def objective(trial):\n",
    "    method = trial.suggest_categorical('method', ['tree', 'knn', 'random_forest', 'gradient_boosting'])\n",
    "    if method== 'tree':\n",
    "        max_depth=trial.suggest_int('max_depth', 2, 20)\n",
    "        min_samples_split= trial.suggest_int('min_samples_split', 50, 150, step=10)\n",
    "        params = {'max_depth': max_depth, 'min_samples_split': min_samples_split}\n",
    "        regr = DecisionTreeRegressor(random_state=random_state, **params)\n",
    "        \n",
    "    elif method=='knn':\n",
    "        n_neighbors= trial.suggest_int('n_neighbors',1,50)\n",
    "        weights = trial.suggest_categorical('weights', ['uniform', 'distance'])\n",
    "        algorithm = trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute'])\n",
    "        metric= trial.suggest_categorical('metric', ['minkowski','euclidean', 'chebyshev', 'manhattan'])\n",
    "\n",
    "        if metric == 'minkowski':\n",
    "            p= trial.suggest_float('p', 1.0,3.0)\n",
    "            params = {'n_neighbors': n_neighbors,'weights': weights,'algorithm': algorithm, 'metric': metric, 'p': p }\n",
    "        else:\n",
    "            params = {'n_neighbors': n_neighbors,'weights': weights,'algorithm': algorithm, 'metric': metric}\n",
    "        \n",
    "        regr = KNeighborsRegressor(**params)\n",
    "\n",
    "    elif method=='random_forest':\n",
    "        max_depth=trial.suggest_int('max_depth', 2, 20)\n",
    "        min_samples_split= trial.suggest_int('min_samples_split', 50, 150,step= 10)\n",
    "        n_estimators= trial.suggest_int('n_estimators', 50, 150,step=10)\n",
    "        max_features= trial.suggest_float('max_features', 0.2, 1.0)\n",
    "        params = {'max_depth': max_depth, 'min_samples_split': min_samples_split,'n_estimators': n_estimators, 'max_features': max_features }\n",
    "        regr = RandomForestRegressor(random_state=random_state, **params)\n",
    "\n",
    "    elif method=='gradient_boosting':\n",
    "        max_depth=trial.suggest_int('max_depth', 2, 20)\n",
    "        min_samples_split= trial.suggest_int('min_samples_split', 50, 150, step=10)\n",
    "        n_estimators= trial.suggest_int('n_estimators', 50, 150, step=10)\n",
    "        learning_rate= trial.suggest_float('learning_rate', 0.01, 0.21, step=0.02)\n",
    "        params = {'max_depth': max_depth, 'min_samples_split': min_samples_split,'n_estimators': n_estimators, 'learning_rate': learning_rate }\n",
    "        regr = GradientBoostingRegressor(random_state=random_state, **params)\n",
    "\n",
    "    scores= cross_val_score(regr, X_train, y_train, scoring= 'neg_mean_absolute_error',\n",
    "                            n_jobs=-1, cv=inner)\n",
    "    return scores.mean()\n",
    "\n",
    "budget=15\n",
    "sampler= optuna.samplers.TPESampler(seed=random_state)\n",
    "study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "start_time = time.time()\n",
    "\n",
    "study.optimize(objective,n_trials=budget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd3ff8b-3554-472e-bc5c-8c09182f087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate training time\n",
    "training_time = end_time - start_time\n",
    "print(study.best_params)\n",
    "print(-study.best_value)\n",
    "print(training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c866a38-51cc-4204-9fca-5b2c07e100b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550dba40-04df-4ba0-8490-008a4977ccb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fd87d09-6faf-476d-bef7-6660aea3d2e6",
   "metadata": {},
   "source": [
    "## 5. Best model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbb6de5-36a3-4164-ba30-5f1162cae1dc",
   "metadata": {},
   "source": [
    "### 5.1. Estimation of the accuracy\n",
    "\n",
    "Using the best alternative (based on inner evaluation), make an estimation of the accuracy that the final model might get on future data (outer evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd530bc3-32e4-48c1-bbe4-f91f5d06dcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=halving_search_gb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6f900d2-f4b2-4eef-8b05-1e08ae1e5b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 282.4050298313833\n"
     ]
    }
   ],
   "source": [
    "mae = mean_absolute_error(y_test, predictions)\n",
    "\n",
    "print(f\"MAE: {mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a14e8d-6fee-4ff4-baee-0bb1f9fb3d43",
   "metadata": {},
   "source": [
    "### 5.2. Training final model and predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24322670-60b9-4545-b0d2-76a29d58f5ba",
   "metadata": {},
   "source": [
    "Train (using ALL THE DATA) and use it to make predictions on the “competition data”. Save both the final model and the competition predictions on files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46736cee-da37-41f6-9d8c-16042b3ef69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wind_ava.drop('energy', axis=1)\n",
    "y=wind_ava['energy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ed87e2-4444-4929-8954-ef6058f43732",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbdfd686-a142-4ff6-9222-c9ec491ee948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "gb_params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 3,\n",
    "    'min_samples_split': 5,\n",
    "    'n_estimators': 100\n",
    "}\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline_final = Pipeline([\n",
    "    ('imputer', IterativeImputer()), \n",
    "    ('scaler', RobustScaler()),\n",
    "    ('feature_selection', SelectKBest(k=150, score_func=mutual_info_regression)),  \n",
    "    ('gb', GradientBoostingRegressor(**gb_params))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c350fbf-d59f-44ca-b10c-aca7bbdab71b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpipeline_final\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/sklearn/pipeline.py:416\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \n\u001b[1;32m    392\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    415\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m--> 416\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/sklearn/pipeline.py:370\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    368\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 370\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/joblib/memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/sklearn/pipeline.py:950\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 950\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    952\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/sklearn/impute/_iterative.py:761\u001b[0m, in \u001b[0;36mIterativeImputer.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feat_idx \u001b[38;5;129;01min\u001b[39;00m ordered_idx:\n\u001b[1;32m    758\u001b[0m     neighbor_feat_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_neighbor_feat_idx(\n\u001b[1;32m    759\u001b[0m         n_features, feat_idx, abs_corr_mat\n\u001b[1;32m    760\u001b[0m     )\n\u001b[0;32m--> 761\u001b[0m     Xt, estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_impute_one_feature\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_missing_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeat_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneighbor_feat_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m     estimator_triplet \u001b[38;5;241m=\u001b[39m _ImputerTriplet(\n\u001b[1;32m    770\u001b[0m         feat_idx, neighbor_feat_idx, estimator\n\u001b[1;32m    771\u001b[0m     )\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimputation_sequence_\u001b[38;5;241m.\u001b[39mappend(estimator_triplet)\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/sklearn/impute/_iterative.py:408\u001b[0m, in \u001b[0;36mIterativeImputer._impute_one_feature\u001b[0;34m(self, X_filled, mask_missing_values, feat_idx, neighbor_feat_idx, estimator, fit_mode)\u001b[0m\n\u001b[1;32m    398\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m _safe_indexing(\n\u001b[1;32m    399\u001b[0m         _safe_indexing(X_filled, neighbor_feat_idx, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    400\u001b[0m         \u001b[38;5;241m~\u001b[39mmissing_row_mask,\n\u001b[1;32m    401\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    402\u001b[0m     )\n\u001b[1;32m    403\u001b[0m     y_train \u001b[38;5;241m=\u001b[39m _safe_indexing(\n\u001b[1;32m    404\u001b[0m         _safe_indexing(X_filled, feat_idx, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;241m~\u001b[39mmissing_row_mask,\n\u001b[1;32m    406\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    407\u001b[0m     )\n\u001b[0;32m--> 408\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# if no missing values, don't predict\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(missing_row_mask) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/sklearn/linear_model/_bayes.py:337\u001b[0m, in \u001b[0;36mBayesianRidge.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    334\u001b[0m coef_old_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    336\u001b[0m XT_y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X\u001b[38;5;241m.\u001b[39mT, y)\n\u001b[0;32m--> 337\u001b[0m U, S, Vh \u001b[38;5;241m=\u001b[39m \u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m eigen_vals_ \u001b[38;5;241m=\u001b[39m S\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# Convergence loop of the bayesian ridge regression\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/scipy/linalg/_decomp_svd.py:127\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m    123\u001b[0m lwork \u001b[38;5;241m=\u001b[39m _compute_lwork(gesXd_lwork, a1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], a1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    124\u001b[0m                        compute_uv\u001b[38;5;241m=\u001b[39mcompute_uv, full_matrices\u001b[38;5;241m=\u001b[39mfull_matrices)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# perform decomposition\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m u, s, v, info \u001b[38;5;241m=\u001b[39m \u001b[43mgesXd\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_uv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_uv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_matrices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_a\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_a\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVD did not converge\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipeline_final.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b88107ff-3743-49be-becc-817d6b8770e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_competition = pd.read_csv(\"/Users/pameladiaz/Desktop/GitHub/Big_Data_Intelligence/wind_competition.csv.gzip\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33df89fe-63f4-4255-a61a-7738cd58ffff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>p54.162.1</th>\n",
       "      <th>p54.162.2</th>\n",
       "      <th>p54.162.3</th>\n",
       "      <th>p54.162.4</th>\n",
       "      <th>p54.162.5</th>\n",
       "      <th>p54.162.6</th>\n",
       "      <th>...</th>\n",
       "      <th>v100.16</th>\n",
       "      <th>v100.17</th>\n",
       "      <th>v100.18</th>\n",
       "      <th>v100.19</th>\n",
       "      <th>v100.20</th>\n",
       "      <th>v100.21</th>\n",
       "      <th>v100.22</th>\n",
       "      <th>v100.23</th>\n",
       "      <th>v100.24</th>\n",
       "      <th>v100.25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.403131e+06</td>\n",
       "      <td>2.395445e+06</td>\n",
       "      <td>2.387755e+06</td>\n",
       "      <td>2.380065e+06</td>\n",
       "      <td>2.372380e+06</td>\n",
       "      <td>2.399548e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>7.212586</td>\n",
       "      <td>7.057422</td>\n",
       "      <td>6.901760</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.591434</td>\n",
       "      <td>7.184147</td>\n",
       "      <td>7.030980</td>\n",
       "      <td>6.877313</td>\n",
       "      <td>6.723647</td>\n",
       "      <td>6.570479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2.410306e+06</td>\n",
       "      <td>2.402394e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.386571e+06</td>\n",
       "      <td>2.378660e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.207289</td>\n",
       "      <td>0.583972</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.714518</td>\n",
       "      <td>0.345988</td>\n",
       "      <td>0.723170</td>\n",
       "      <td>1.100850</td>\n",
       "      <td>1.478031</td>\n",
       "      <td>1.855712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2.434908e+06</td>\n",
       "      <td>2.426793e+06</td>\n",
       "      <td>2.418683e+06</td>\n",
       "      <td>2.410573e+06</td>\n",
       "      <td>2.402462e+06</td>\n",
       "      <td>2.431465e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.670114</td>\n",
       "      <td>1.691568</td>\n",
       "      <td>1.712522</td>\n",
       "      <td>1.733976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.664127</td>\n",
       "      <td>1.682587</td>\n",
       "      <td>1.700548</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.736470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>2.447112e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.431027e+06</td>\n",
       "      <td>2.422984e+06</td>\n",
       "      <td>2.414942e+06</td>\n",
       "      <td>2.443696e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.217597</td>\n",
       "      <td>1.278464</td>\n",
       "      <td>1.339332</td>\n",
       "      <td>1.399701</td>\n",
       "      <td>1.460569</td>\n",
       "      <td>1.215102</td>\n",
       "      <td>1.272477</td>\n",
       "      <td>1.329853</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.444105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.459695e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.443809e+06</td>\n",
       "      <td>2.435866e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.456252e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>3.755089</td>\n",
       "      <td>3.686738</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.549536</td>\n",
       "      <td>3.481184</td>\n",
       "      <td>3.781532</td>\n",
       "      <td>3.710686</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.569492</td>\n",
       "      <td>3.498646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>2010</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>18</td>\n",
       "      <td>2.450279e+06</td>\n",
       "      <td>2.442801e+06</td>\n",
       "      <td>2.435324e+06</td>\n",
       "      <td>2.427846e+06</td>\n",
       "      <td>2.420368e+06</td>\n",
       "      <td>2.446240e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>3.473201</td>\n",
       "      <td>3.445761</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.391379</td>\n",
       "      <td>3.363938</td>\n",
       "      <td>3.499644</td>\n",
       "      <td>3.467214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.401856</td>\n",
       "      <td>3.369426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>2010</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>2.455407e+06</td>\n",
       "      <td>2.447817e+06</td>\n",
       "      <td>2.440226e+06</td>\n",
       "      <td>2.432635e+06</td>\n",
       "      <td>2.425045e+06</td>\n",
       "      <td>2.451400e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.280789</td>\n",
       "      <td>2.372091</td>\n",
       "      <td>2.463892</td>\n",
       "      <td>2.555194</td>\n",
       "      <td>2.646994</td>\n",
       "      <td>2.333674</td>\n",
       "      <td>2.418490</td>\n",
       "      <td>2.503306</td>\n",
       "      <td>2.588621</td>\n",
       "      <td>2.673437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>2010</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>2.457296e+06</td>\n",
       "      <td>2.449624e+06</td>\n",
       "      <td>2.441947e+06</td>\n",
       "      <td>2.434271e+06</td>\n",
       "      <td>2.426599e+06</td>\n",
       "      <td>2.453288e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.211939</td>\n",
       "      <td>2.341657</td>\n",
       "      <td>2.470877</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.729815</td>\n",
       "      <td>2.188489</td>\n",
       "      <td>2.312221</td>\n",
       "      <td>2.436451</td>\n",
       "      <td>2.560183</td>\n",
       "      <td>2.684413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>2010</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>2.464015e+06</td>\n",
       "      <td>2.456257e+06</td>\n",
       "      <td>2.448494e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.432974e+06</td>\n",
       "      <td>2.459993e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.311393</td>\n",
       "      <td>1.387228</td>\n",
       "      <td>1.463563</td>\n",
       "      <td>1.539897</td>\n",
       "      <td>1.263996</td>\n",
       "      <td>1.338335</td>\n",
       "      <td>1.412174</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.560852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1188</th>\n",
       "      <td>2010</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>18</td>\n",
       "      <td>2.468054e+06</td>\n",
       "      <td>2.460197e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.444478e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.464019e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.887335</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.636879</td>\n",
       "      <td>-0.512149</td>\n",
       "      <td>-0.386921</td>\n",
       "      <td>-0.840437</td>\n",
       "      <td>-0.716705</td>\n",
       "      <td>-0.592974</td>\n",
       "      <td>-0.469242</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1189 rows × 554 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      year  month  day  hour     p54.162.1     p54.162.2     p54.162.3  \\\n",
       "0     2010      1    1     0  2.403131e+06  2.395445e+06  2.387755e+06   \n",
       "1     2010      1    1     6  2.410306e+06  2.402394e+06           NaN   \n",
       "2     2010      1    1    12  2.434908e+06  2.426793e+06  2.418683e+06   \n",
       "3     2010      1    1    18  2.447112e+06           NaN  2.431027e+06   \n",
       "4     2010      1    2     0  2.459695e+06           NaN  2.443809e+06   \n",
       "...    ...    ...  ...   ...           ...           ...           ...   \n",
       "1184  2010     12   30    18  2.450279e+06  2.442801e+06  2.435324e+06   \n",
       "1185  2010     12   31     0  2.455407e+06  2.447817e+06  2.440226e+06   \n",
       "1186  2010     12   31     6  2.457296e+06  2.449624e+06  2.441947e+06   \n",
       "1187  2010     12   31    12  2.464015e+06  2.456257e+06  2.448494e+06   \n",
       "1188  2010     12   31    18  2.468054e+06  2.460197e+06           NaN   \n",
       "\n",
       "         p54.162.4     p54.162.5     p54.162.6  ...   v100.16   v100.17  \\\n",
       "0     2.380065e+06  2.372380e+06  2.399548e+06  ...  7.212586  7.057422   \n",
       "1     2.386571e+06  2.378660e+06           NaN  ...  0.207289  0.583972   \n",
       "2     2.410573e+06  2.402462e+06  2.431465e+06  ...  1.670114  1.691568   \n",
       "3     2.422984e+06  2.414942e+06  2.443696e+06  ...  1.217597  1.278464   \n",
       "4     2.435866e+06           NaN  2.456252e+06  ...  3.755089  3.686738   \n",
       "...            ...           ...           ...  ...       ...       ...   \n",
       "1184  2.427846e+06  2.420368e+06  2.446240e+06  ...  3.473201  3.445761   \n",
       "1185  2.432635e+06  2.425045e+06  2.451400e+06  ...  2.280789  2.372091   \n",
       "1186  2.434271e+06  2.426599e+06  2.453288e+06  ...  2.211939  2.341657   \n",
       "1187           NaN  2.432974e+06  2.459993e+06  ...       NaN  1.311393   \n",
       "1188  2.444478e+06           NaN  2.464019e+06  ... -0.887335       NaN   \n",
       "\n",
       "       v100.18   v100.19   v100.20   v100.21   v100.22   v100.23   v100.24  \\\n",
       "0     6.901760       NaN  6.591434  7.184147  7.030980  6.877313  6.723647   \n",
       "1          NaN       NaN  1.714518  0.345988  0.723170  1.100850  1.478031   \n",
       "2     1.712522  1.733976       NaN  1.664127  1.682587  1.700548       NaN   \n",
       "3     1.339332  1.399701  1.460569  1.215102  1.272477  1.329853       NaN   \n",
       "4          NaN  3.549536  3.481184  3.781532  3.710686       NaN  3.569492   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1184       NaN  3.391379  3.363938  3.499644  3.467214       NaN  3.401856   \n",
       "1185  2.463892  2.555194  2.646994  2.333674  2.418490  2.503306  2.588621   \n",
       "1186  2.470877       NaN  2.729815  2.188489  2.312221  2.436451  2.560183   \n",
       "1187  1.387228  1.463563  1.539897  1.263996  1.338335  1.412174       NaN   \n",
       "1188 -0.636879 -0.512149 -0.386921 -0.840437 -0.716705 -0.592974 -0.469242   \n",
       "\n",
       "       v100.25  \n",
       "0     6.570479  \n",
       "1     1.855712  \n",
       "2     1.736470  \n",
       "3     1.444105  \n",
       "4     3.498646  \n",
       "...        ...  \n",
       "1184  3.369426  \n",
       "1185  2.673437  \n",
       "1186  2.684413  \n",
       "1187  1.560852  \n",
       "1188       NaN  \n",
       "\n",
       "[1189 rows x 554 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "wind_competition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a89f1a-28a3-427d-a483-11859962bdf4",
   "metadata": {},
   "source": [
    "### 5.3. Model interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e12d7bd-0e0c-4a96-89f2-a4b17560cf6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fc87cf0-02d7-4b69-a26b-3e5a66e95521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    },
    {
     "ename": "InvalidModelError",
     "evalue": "Model type not yet supported by TreeExplainer: <class 'sklearn.model_selection._search_successive_halving.HalvingGridSearchCV'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidModelError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming model is your trained gradient boosting model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m explainer \u001b[38;5;241m=\u001b[39m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTreeExplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhalving_search_gb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mshap_values(X_train)  \u001b[38;5;66;03m# X is your input data\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Plot summary plot\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/shap/explainers/_tree.py:166\u001b[0m, in \u001b[0;36mTree.__init__\u001b[0;34m(self, model, data, model_output, feature_perturbation, feature_names, approximate, **deprecated_options)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_perturbation \u001b[38;5;241m=\u001b[39m feature_perturbation\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpected_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mTreeEnsemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_missing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_output \u001b[38;5;241m=\u001b[39m model_output\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m#self.model_output = self.model.model_output # this allows the TreeEnsemble to translate model outputs types by how it loads the model\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/shap/explainers/_tree.py:1133\u001b[0m, in \u001b[0;36mTreeEnsemble.__init__\u001b[0;34m(self, model, data, data_missing, model_output)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_offset \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minit_params[param_idx]\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidModelError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type not yet supported by TreeExplainer: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(model)))\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# build a dense numpy version of all the tree objects\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrees \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrees:\n",
      "\u001b[0;31mInvalidModelError\u001b[0m: Model type not yet supported by TreeExplainer: <class 'sklearn.model_selection._search_successive_halving.HalvingGridSearchCV'>"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "# Assuming model is your trained gradient boosting model\n",
    "explainer = shap.TreeExplainer(halving_search_gb)\n",
    "shap_values = explainer.shap_values(X_train)  # X is your input data\n",
    "\n",
    "# Plot summary plot\n",
    "shap.summary_plot(shap_values, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5019eaaf-c78f-4079-a6c6-9c4c5dce9c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p59.162.1</th>\n",
       "      <th>p59.162.2</th>\n",
       "      <th>p59.162.3</th>\n",
       "      <th>p59.162.4</th>\n",
       "      <th>p59.162.5</th>\n",
       "      <th>p59.162.6</th>\n",
       "      <th>p59.162.7</th>\n",
       "      <th>p59.162.9</th>\n",
       "      <th>p59.162.11</th>\n",
       "      <th>p59.162.12</th>\n",
       "      <th>...</th>\n",
       "      <th>wind_speed16</th>\n",
       "      <th>wind_speed17</th>\n",
       "      <th>wind_speed18</th>\n",
       "      <th>wind_speed19</th>\n",
       "      <th>wind_speed20</th>\n",
       "      <th>wind_speed21</th>\n",
       "      <th>wind_speed22</th>\n",
       "      <th>wind_speed23</th>\n",
       "      <th>wind_speed24</th>\n",
       "      <th>wind_speed25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.400898e+06</td>\n",
       "      <td>1.405015e+06</td>\n",
       "      <td>1.408953e+06</td>\n",
       "      <td>1.413070e+06</td>\n",
       "      <td>1.417008e+06</td>\n",
       "      <td>1.389443e+06</td>\n",
       "      <td>1.393023e+06</td>\n",
       "      <td>1.400362e+06</td>\n",
       "      <td>1.380314e+06</td>\n",
       "      <td>1.383536e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>5.134636</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.393808</td>\n",
       "      <td>5.137268</td>\n",
       "      <td>4.948249</td>\n",
       "      <td>4.760744</td>\n",
       "      <td>4.574870</td>\n",
       "      <td>4.389944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.167130e+06</td>\n",
       "      <td>1.171426e+06</td>\n",
       "      <td>1.175722e+06</td>\n",
       "      <td>1.180018e+06</td>\n",
       "      <td>1.184314e+06</td>\n",
       "      <td>1.155674e+06</td>\n",
       "      <td>1.159433e+06</td>\n",
       "      <td>1.167130e+06</td>\n",
       "      <td>1.146367e+06</td>\n",
       "      <td>1.149946e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>5.298553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.869726</td>\n",
       "      <td>4.655077</td>\n",
       "      <td>4.440835</td>\n",
       "      <td>5.300003</td>\n",
       "      <td>5.085998</td>\n",
       "      <td>4.872320</td>\n",
       "      <td>4.658335</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.114147e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.115758e+06</td>\n",
       "      <td>1.116653e+06</td>\n",
       "      <td>1.104661e+06</td>\n",
       "      <td>1.105377e+06</td>\n",
       "      <td>1.106450e+06</td>\n",
       "      <td>1.098038e+06</td>\n",
       "      <td>1.098396e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>5.587899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.529130</td>\n",
       "      <td>5.324473</td>\n",
       "      <td>5.122190</td>\n",
       "      <td>4.922326</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.090699e+06</td>\n",
       "      <td>1.090878e+06</td>\n",
       "      <td>1.091057e+06</td>\n",
       "      <td>1.091236e+06</td>\n",
       "      <td>1.091594e+06</td>\n",
       "      <td>1.082644e+06</td>\n",
       "      <td>1.082644e+06</td>\n",
       "      <td>1.082823e+06</td>\n",
       "      <td>1.076200e+06</td>\n",
       "      <td>1.076200e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>4.779456</td>\n",
       "      <td>4.715311</td>\n",
       "      <td>4.655777</td>\n",
       "      <td>4.600019</td>\n",
       "      <td>4.549180</td>\n",
       "      <td>4.698198</td>\n",
       "      <td>4.634036</td>\n",
       "      <td>4.573135</td>\n",
       "      <td>4.516475</td>\n",
       "      <td>4.463542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.093563e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.081749e+06</td>\n",
       "      <td>1.077811e+06</td>\n",
       "      <td>1.085329e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.078885e+06</td>\n",
       "      <td>1.074947e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>3.941046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.917355</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.851946</td>\n",
       "      <td>3.843097</td>\n",
       "      <td>3.836565</td>\n",
       "      <td>3.832704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3644</th>\n",
       "      <td>8.506655e+05</td>\n",
       "      <td>8.427897e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.272171e+05</td>\n",
       "      <td>8.193413e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.171933e+05</td>\n",
       "      <td>8.023367e+05</td>\n",
       "      <td>8.043056e+05</td>\n",
       "      <td>7.969668e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.471601</td>\n",
       "      <td>7.279755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.895840</td>\n",
       "      <td>7.371665</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.981368</td>\n",
       "      <td>6.786564</td>\n",
       "      <td>6.591311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3645</th>\n",
       "      <td>1.327868e+06</td>\n",
       "      <td>1.311759e+06</td>\n",
       "      <td>1.295828e+06</td>\n",
       "      <td>1.279718e+06</td>\n",
       "      <td>1.263609e+06</td>\n",
       "      <td>1.294575e+06</td>\n",
       "      <td>1.278644e+06</td>\n",
       "      <td>1.246783e+06</td>\n",
       "      <td>1.268263e+06</td>\n",
       "      <td>1.252511e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.912081</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.633936</td>\n",
       "      <td>6.496771</td>\n",
       "      <td>6.868302</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.577254</td>\n",
       "      <td>6.433403</td>\n",
       "      <td>6.290071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3646</th>\n",
       "      <td>1.630013e+06</td>\n",
       "      <td>1.622495e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.607460e+06</td>\n",
       "      <td>1.599942e+06</td>\n",
       "      <td>1.618199e+06</td>\n",
       "      <td>1.609966e+06</td>\n",
       "      <td>1.593319e+06</td>\n",
       "      <td>1.608713e+06</td>\n",
       "      <td>1.599942e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>5.251092</td>\n",
       "      <td>5.224762</td>\n",
       "      <td>5.200617</td>\n",
       "      <td>5.177566</td>\n",
       "      <td>5.155690</td>\n",
       "      <td>5.070743</td>\n",
       "      <td>5.042703</td>\n",
       "      <td>5.016349</td>\n",
       "      <td>4.991320</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3647</th>\n",
       "      <td>4.058611e+05</td>\n",
       "      <td>4.051451e+05</td>\n",
       "      <td>4.042501e+05</td>\n",
       "      <td>4.035342e+05</td>\n",
       "      <td>4.028182e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.004912e+05</td>\n",
       "      <td>3.990593e+05</td>\n",
       "      <td>3.976273e+05</td>\n",
       "      <td>3.969113e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.655181</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.487387</td>\n",
       "      <td>2.663482</td>\n",
       "      <td>2.607470</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.495662</td>\n",
       "      <td>2.439739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3648</th>\n",
       "      <td>4.966120e+05</td>\n",
       "      <td>4.921371e+05</td>\n",
       "      <td>4.874832e+05</td>\n",
       "      <td>4.828293e+05</td>\n",
       "      <td>4.781754e+05</td>\n",
       "      <td>4.912421e+05</td>\n",
       "      <td>4.867672e+05</td>\n",
       "      <td>4.774594e+05</td>\n",
       "      <td>4.869462e+05</td>\n",
       "      <td>4.824713e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.639209</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.736581</td>\n",
       "      <td>2.786005</td>\n",
       "      <td>2.836004</td>\n",
       "      <td>2.570666</td>\n",
       "      <td>2.615252</td>\n",
       "      <td>2.660920</td>\n",
       "      <td>2.706500</td>\n",
       "      <td>2.752172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3649 rows × 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         p59.162.1     p59.162.2     p59.162.3     p59.162.4     p59.162.5  \\\n",
       "0     1.400898e+06  1.405015e+06  1.408953e+06  1.413070e+06  1.417008e+06   \n",
       "1     1.167130e+06  1.171426e+06  1.175722e+06  1.180018e+06  1.184314e+06   \n",
       "2              NaN  1.114147e+06           NaN  1.115758e+06  1.116653e+06   \n",
       "3     1.090699e+06  1.090878e+06  1.091057e+06  1.091236e+06  1.091594e+06   \n",
       "4     1.093563e+06           NaN           NaN  1.081749e+06  1.077811e+06   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "3644  8.506655e+05  8.427897e+05           NaN  8.272171e+05  8.193413e+05   \n",
       "3645  1.327868e+06  1.311759e+06  1.295828e+06  1.279718e+06  1.263609e+06   \n",
       "3646  1.630013e+06  1.622495e+06           NaN  1.607460e+06  1.599942e+06   \n",
       "3647  4.058611e+05  4.051451e+05  4.042501e+05  4.035342e+05  4.028182e+05   \n",
       "3648  4.966120e+05  4.921371e+05  4.874832e+05  4.828293e+05  4.781754e+05   \n",
       "\n",
       "         p59.162.6     p59.162.7     p59.162.9    p59.162.11    p59.162.12  \\\n",
       "0     1.389443e+06  1.393023e+06  1.400362e+06  1.380314e+06  1.383536e+06   \n",
       "1     1.155674e+06  1.159433e+06  1.167130e+06  1.146367e+06  1.149946e+06   \n",
       "2     1.104661e+06  1.105377e+06  1.106450e+06  1.098038e+06  1.098396e+06   \n",
       "3     1.082644e+06  1.082644e+06  1.082823e+06  1.076200e+06  1.076200e+06   \n",
       "4     1.085329e+06           NaN           NaN  1.078885e+06  1.074947e+06   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "3644           NaN  8.171933e+05  8.023367e+05  8.043056e+05  7.969668e+05   \n",
       "3645  1.294575e+06  1.278644e+06  1.246783e+06  1.268263e+06  1.252511e+06   \n",
       "3646  1.618199e+06  1.609966e+06  1.593319e+06  1.608713e+06  1.599942e+06   \n",
       "3647           NaN  4.004912e+05  3.990593e+05  3.976273e+05  3.969113e+05   \n",
       "3648  4.912421e+05  4.867672e+05  4.774594e+05  4.869462e+05  4.824713e+05   \n",
       "\n",
       "      ...  wind_speed16  wind_speed17  wind_speed18  wind_speed19  \\\n",
       "0     ...      5.134636           NaN           NaN           NaN   \n",
       "1     ...      5.298553           NaN      4.869726      4.655077   \n",
       "2     ...      5.587899           NaN           NaN           NaN   \n",
       "3     ...      4.779456      4.715311      4.655777      4.600019   \n",
       "4     ...      3.941046           NaN           NaN      3.917355   \n",
       "...   ...           ...           ...           ...           ...   \n",
       "3644  ...           NaN      7.471601      7.279755           NaN   \n",
       "3645  ...           NaN      6.912081           NaN      6.633936   \n",
       "3646  ...      5.251092      5.224762      5.200617      5.177566   \n",
       "3647  ...           NaN      2.655181           NaN           NaN   \n",
       "3648  ...      2.639209           NaN      2.736581      2.786005   \n",
       "\n",
       "      wind_speed20  wind_speed21  wind_speed22  wind_speed23  wind_speed24  \\\n",
       "0         4.393808      5.137268      4.948249      4.760744      4.574870   \n",
       "1         4.440835      5.300003      5.085998      4.872320      4.658335   \n",
       "2              NaN      5.529130      5.324473      5.122190      4.922326   \n",
       "3         4.549180      4.698198      4.634036      4.573135      4.516475   \n",
       "4              NaN           NaN      3.851946      3.843097      3.836565   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "3644      6.895840      7.371665           NaN      6.981368      6.786564   \n",
       "3645      6.496771      6.868302           NaN      6.577254      6.433403   \n",
       "3646      5.155690      5.070743      5.042703      5.016349      4.991320   \n",
       "3647      2.487387      2.663482      2.607470           NaN      2.495662   \n",
       "3648      2.836004      2.570666      2.615252      2.660920      2.706500   \n",
       "\n",
       "      wind_speed25  \n",
       "0         4.389944  \n",
       "1              NaN  \n",
       "2              NaN  \n",
       "3         4.463542  \n",
       "4         3.832704  \n",
       "...            ...  \n",
       "3644      6.591311  \n",
       "3645      6.290071  \n",
       "3646           NaN  \n",
       "3647      2.439739  \n",
       "3648      2.752172  \n",
       "\n",
       "[3649 rows x 250 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only the headers from the original DataFrame\n",
    "selected_feature_headers = X_selected_features.columns\n",
    "\n",
    "# Create a new DataFrame using the selected headers and columns of X_train\n",
    "X_prueba = pd.DataFrame(X_train, columns=selected_feature_headers)\n",
    "X_prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c8b5807-4e82-41db-84a7-ddf20a7910db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p59.162.1</th>\n",
       "      <th>p59.162.2</th>\n",
       "      <th>p59.162.3</th>\n",
       "      <th>p59.162.4</th>\n",
       "      <th>p59.162.5</th>\n",
       "      <th>p59.162.6</th>\n",
       "      <th>p59.162.7</th>\n",
       "      <th>p59.162.9</th>\n",
       "      <th>p59.162.11</th>\n",
       "      <th>p59.162.12</th>\n",
       "      <th>...</th>\n",
       "      <th>wind_speed16</th>\n",
       "      <th>wind_speed17</th>\n",
       "      <th>wind_speed18</th>\n",
       "      <th>wind_speed19</th>\n",
       "      <th>wind_speed20</th>\n",
       "      <th>wind_speed21</th>\n",
       "      <th>wind_speed22</th>\n",
       "      <th>wind_speed23</th>\n",
       "      <th>wind_speed24</th>\n",
       "      <th>wind_speed25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3649</th>\n",
       "      <td>7.187457e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.911803e+05</td>\n",
       "      <td>6.775767e+05</td>\n",
       "      <td>6.637940e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.006671e+05</td>\n",
       "      <td>6.734598e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.974452e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>4.860973</td>\n",
       "      <td>4.804251</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.691537</td>\n",
       "      <td>4.635573</td>\n",
       "      <td>4.805741</td>\n",
       "      <td>4.750617</td>\n",
       "      <td>4.695948</td>\n",
       "      <td>4.641535</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3650</th>\n",
       "      <td>1.064565e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.025365e+06</td>\n",
       "      <td>1.005676e+06</td>\n",
       "      <td>9.861653e+05</td>\n",
       "      <td>1.060807e+06</td>\n",
       "      <td>1.041117e+06</td>\n",
       "      <td>1.002096e+06</td>\n",
       "      <td>1.057764e+06</td>\n",
       "      <td>1.038253e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>6.398697</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.839055</td>\n",
       "      <td>5.653117</td>\n",
       "      <td>6.311038</td>\n",
       "      <td>6.128136</td>\n",
       "      <td>5.945880</td>\n",
       "      <td>5.763317</td>\n",
       "      <td>5.580927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3651</th>\n",
       "      <td>1.302988e+06</td>\n",
       "      <td>1.283656e+06</td>\n",
       "      <td>1.264325e+06</td>\n",
       "      <td>1.244993e+06</td>\n",
       "      <td>1.225662e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.281687e+06</td>\n",
       "      <td>1.242666e+06</td>\n",
       "      <td>1.299766e+06</td>\n",
       "      <td>1.280076e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>9.014981</td>\n",
       "      <td>8.627820</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.854394</td>\n",
       "      <td>7.468168</td>\n",
       "      <td>8.891314</td>\n",
       "      <td>8.505001</td>\n",
       "      <td>8.118789</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3652</th>\n",
       "      <td>1.979592e+06</td>\n",
       "      <td>1.914616e+06</td>\n",
       "      <td>1.849641e+06</td>\n",
       "      <td>1.784665e+06</td>\n",
       "      <td>1.719690e+06</td>\n",
       "      <td>1.985499e+06</td>\n",
       "      <td>1.920165e+06</td>\n",
       "      <td>1.789319e+06</td>\n",
       "      <td>1.990331e+06</td>\n",
       "      <td>1.924461e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>11.676951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.735430</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.805992</td>\n",
       "      <td>11.542441</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.604058</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.676562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3653</th>\n",
       "      <td>2.315567e+06</td>\n",
       "      <td>2.285137e+06</td>\n",
       "      <td>2.254708e+06</td>\n",
       "      <td>2.224458e+06</td>\n",
       "      <td>2.194029e+06</td>\n",
       "      <td>2.323264e+06</td>\n",
       "      <td>2.292834e+06</td>\n",
       "      <td>2.231976e+06</td>\n",
       "      <td>2.329528e+06</td>\n",
       "      <td>2.298920e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>9.273835</td>\n",
       "      <td>9.063486</td>\n",
       "      <td>8.862578</td>\n",
       "      <td>8.671693</td>\n",
       "      <td>8.492027</td>\n",
       "      <td>9.168775</td>\n",
       "      <td>8.959526</td>\n",
       "      <td>8.759649</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.390777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4743</th>\n",
       "      <td>3.611317e+06</td>\n",
       "      <td>3.699920e+06</td>\n",
       "      <td>3.788523e+06</td>\n",
       "      <td>3.877126e+06</td>\n",
       "      <td>3.965729e+06</td>\n",
       "      <td>3.674324e+06</td>\n",
       "      <td>3.764179e+06</td>\n",
       "      <td>3.943533e+06</td>\n",
       "      <td>3.724442e+06</td>\n",
       "      <td>3.815014e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>11.698059</td>\n",
       "      <td>11.580590</td>\n",
       "      <td>11.463558</td>\n",
       "      <td>11.346166</td>\n",
       "      <td>11.228817</td>\n",
       "      <td>11.704657</td>\n",
       "      <td>11.583126</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.340611</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4744</th>\n",
       "      <td>2.514968e+06</td>\n",
       "      <td>2.593368e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.828390e+06</td>\n",
       "      <td>2.554705e+06</td>\n",
       "      <td>2.633821e+06</td>\n",
       "      <td>2.792053e+06</td>\n",
       "      <td>2.586029e+06</td>\n",
       "      <td>2.665861e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>10.444271</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.565530</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.696277</td>\n",
       "      <td>10.478256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.580514</td>\n",
       "      <td>10.635079</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4745</th>\n",
       "      <td>2.073207e+06</td>\n",
       "      <td>2.128158e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.238062e+06</td>\n",
       "      <td>2.293192e+06</td>\n",
       "      <td>2.110975e+06</td>\n",
       "      <td>2.166821e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.197430e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>4.515215</td>\n",
       "      <td>4.541995</td>\n",
       "      <td>4.569241</td>\n",
       "      <td>4.596944</td>\n",
       "      <td>4.625096</td>\n",
       "      <td>4.418757</td>\n",
       "      <td>4.446906</td>\n",
       "      <td>4.475020</td>\n",
       "      <td>4.503584</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4746</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.359908e+06</td>\n",
       "      <td>1.358297e+06</td>\n",
       "      <td>1.356687e+06</td>\n",
       "      <td>1.355076e+06</td>\n",
       "      <td>1.371543e+06</td>\n",
       "      <td>1.370111e+06</td>\n",
       "      <td>1.367247e+06</td>\n",
       "      <td>1.379598e+06</td>\n",
       "      <td>1.378345e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>9.462473</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.976908</td>\n",
       "      <td>9.502296</td>\n",
       "      <td>9.123884</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.383905</td>\n",
       "      <td>8.024426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747</th>\n",
       "      <td>1.323572e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.329300e+06</td>\n",
       "      <td>1.332164e+06</td>\n",
       "      <td>1.335028e+06</td>\n",
       "      <td>1.339682e+06</td>\n",
       "      <td>1.342188e+06</td>\n",
       "      <td>1.347021e+06</td>\n",
       "      <td>1.352570e+06</td>\n",
       "      <td>1.354718e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>7.425060</td>\n",
       "      <td>7.361131</td>\n",
       "      <td>7.297110</td>\n",
       "      <td>7.233272</td>\n",
       "      <td>7.169348</td>\n",
       "      <td>7.413167</td>\n",
       "      <td>7.340337</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.194610</td>\n",
       "      <td>7.121851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1099 rows × 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         p59.162.1     p59.162.2     p59.162.3     p59.162.4     p59.162.5  \\\n",
       "3649  7.187457e+05           NaN  6.911803e+05  6.775767e+05  6.637940e+05   \n",
       "3650  1.064565e+06           NaN  1.025365e+06  1.005676e+06  9.861653e+05   \n",
       "3651  1.302988e+06  1.283656e+06  1.264325e+06  1.244993e+06  1.225662e+06   \n",
       "3652  1.979592e+06  1.914616e+06  1.849641e+06  1.784665e+06  1.719690e+06   \n",
       "3653  2.315567e+06  2.285137e+06  2.254708e+06  2.224458e+06  2.194029e+06   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "4743  3.611317e+06  3.699920e+06  3.788523e+06  3.877126e+06  3.965729e+06   \n",
       "4744  2.514968e+06  2.593368e+06           NaN           NaN  2.828390e+06   \n",
       "4745  2.073207e+06  2.128158e+06           NaN  2.238062e+06  2.293192e+06   \n",
       "4746           NaN  1.359908e+06  1.358297e+06  1.356687e+06  1.355076e+06   \n",
       "4747  1.323572e+06           NaN  1.329300e+06  1.332164e+06  1.335028e+06   \n",
       "\n",
       "         p59.162.6     p59.162.7     p59.162.9    p59.162.11    p59.162.12  \\\n",
       "3649           NaN  7.006671e+05  6.734598e+05           NaN  6.974452e+05   \n",
       "3650  1.060807e+06  1.041117e+06  1.002096e+06  1.057764e+06  1.038253e+06   \n",
       "3651           NaN  1.281687e+06  1.242666e+06  1.299766e+06  1.280076e+06   \n",
       "3652  1.985499e+06  1.920165e+06  1.789319e+06  1.990331e+06  1.924461e+06   \n",
       "3653  2.323264e+06  2.292834e+06  2.231976e+06  2.329528e+06  2.298920e+06   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "4743  3.674324e+06  3.764179e+06  3.943533e+06  3.724442e+06  3.815014e+06   \n",
       "4744  2.554705e+06  2.633821e+06  2.792053e+06  2.586029e+06  2.665861e+06   \n",
       "4745  2.110975e+06  2.166821e+06           NaN           NaN  2.197430e+06   \n",
       "4746  1.371543e+06  1.370111e+06  1.367247e+06  1.379598e+06  1.378345e+06   \n",
       "4747  1.339682e+06  1.342188e+06  1.347021e+06  1.352570e+06  1.354718e+06   \n",
       "\n",
       "      ...  wind_speed16  wind_speed17  wind_speed18  wind_speed19  \\\n",
       "3649  ...      4.860973      4.804251           NaN      4.691537   \n",
       "3650  ...      6.398697           NaN           NaN      5.839055   \n",
       "3651  ...      9.014981      8.627820           NaN      7.854394   \n",
       "3652  ...     11.676951           NaN     10.735430           NaN   \n",
       "3653  ...      9.273835      9.063486      8.862578      8.671693   \n",
       "...   ...           ...           ...           ...           ...   \n",
       "4743  ...     11.698059     11.580590     11.463558     11.346166   \n",
       "4744  ...     10.444271           NaN     10.565530           NaN   \n",
       "4745  ...      4.515215      4.541995      4.569241      4.596944   \n",
       "4746  ...      9.462473           NaN           NaN           NaN   \n",
       "4747  ...      7.425060      7.361131      7.297110      7.233272   \n",
       "\n",
       "      wind_speed20  wind_speed21  wind_speed22  wind_speed23  wind_speed24  \\\n",
       "3649      4.635573      4.805741      4.750617      4.695948      4.641535   \n",
       "3650      5.653117      6.311038      6.128136      5.945880      5.763317   \n",
       "3651      7.468168      8.891314      8.505001      8.118789           NaN   \n",
       "3652      9.805992     11.542441           NaN     10.604058           NaN   \n",
       "3653      8.492027      9.168775      8.959526      8.759649           NaN   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "4743     11.228817     11.704657     11.583126           NaN     11.340611   \n",
       "4744     10.696277     10.478256           NaN     10.580514     10.635079   \n",
       "4745      4.625096      4.418757      4.446906      4.475020      4.503584   \n",
       "4746      7.976908      9.502296      9.123884           NaN      8.383905   \n",
       "4747      7.169348      7.413167      7.340337           NaN      7.194610   \n",
       "\n",
       "      wind_speed25  \n",
       "3649           NaN  \n",
       "3650      5.580927  \n",
       "3651           NaN  \n",
       "3652      9.676562  \n",
       "3653      8.390777  \n",
       "...            ...  \n",
       "4743           NaN  \n",
       "4744           NaN  \n",
       "4745           NaN  \n",
       "4746      8.024426  \n",
       "4747      7.121851  \n",
       "\n",
       "[1099 rows x 250 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new DataFrame using the selected headers and columns of X_train\n",
    "X_prueba_test = pd.DataFrame(X_test, columns=selected_feature_headers)\n",
    "X_prueba_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "368cb193-03a8-4bed-9f4f-cd644094cf3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Domain error in arguments. The `scale` parameter must be positive for all distributions, and many distributions have restrictions on shape parameters. Please see the `scipy.stats.truncnorm` documentation for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m explainer \u001b[38;5;241m=\u001b[39m lime_tabular\u001b[38;5;241m.\u001b[39mLimeTabularExplainer(X_prueba\u001b[38;5;241m.\u001b[39mvalues, feature_names\u001b[38;5;241m=\u001b[39mX_prueba\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Explain a prediction for a specific instance\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m exp \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhalving_search_gb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Visualize the explanation\u001b[39;00m\n\u001b[1;32m     10\u001b[0m exp\u001b[38;5;241m.\u001b[39mshow_in_notebook(show_table\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, show_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/lime/lime_tabular.py:340\u001b[0m, in \u001b[0;36mLimeTabularExplainer.explain_instance\u001b[0;34m(self, data_row, predict_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sp\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39missparse(data_row) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sp\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39misspmatrix_csr(data_row):\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;66;03m# Preventative code: if sparse, convert to csr format if not in csr format already\u001b[39;00m\n\u001b[1;32m    339\u001b[0m     data_row \u001b[38;5;241m=\u001b[39m data_row\u001b[38;5;241m.\u001b[39mtocsr()\n\u001b[0;32m--> 340\u001b[0m data, inverse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__data_inverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_row\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sp\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39missparse(data):\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;66;03m# Note in sparse case we don't subtract mean since data would become dense\u001b[39;00m\n\u001b[1;32m    343\u001b[0m     scaled_data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mmultiply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale_)\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/lime/lime_tabular.py:550\u001b[0m, in \u001b[0;36mLimeTabularExplainer.__data_inverse\u001b[0;34m(self, data_row, num_samples)\u001b[0m\n\u001b[1;32m    548\u001b[0m     inverse[:, column] \u001b[38;5;241m=\u001b[39m inverse_column\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscretizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m     inverse[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscretizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mundiscretize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minverse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m inverse[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m data_row\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, inverse\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/lime/discretize.py:144\u001b[0m, in \u001b[0;36mBaseDiscretizer.undiscretize\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    140\u001b[0m         ret[feature] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_undiscretize_values(\n\u001b[1;32m    141\u001b[0m             feature, ret[feature]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    142\u001b[0m         )\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         ret[:, feature] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_undiscretize_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mret\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/lime/discretize.py:127\u001b[0m, in \u001b[0;36mBaseDiscretizer.get_undiscretize_values\u001b[0;34m(self, feature, values)\u001b[0m\n\u001b[1;32m    124\u001b[0m min_max_unequal \u001b[38;5;241m=\u001b[39m (minz \u001b[38;5;241m!=\u001b[39m maxz)\n\u001b[1;32m    126\u001b[0m ret \u001b[38;5;241m=\u001b[39m minz\n\u001b[0;32m--> 127\u001b[0m ret[np\u001b[38;5;241m.\u001b[39mwhere(min_max_unequal)] \u001b[38;5;241m=\u001b[39m \u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtruncnorm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrvs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mminz\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmin_max_unequal\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaxz\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmin_max_unequal\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeans\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmin_max_unequal\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmin_max_unequal\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/scipy/stats/_distn_infrastructure.py:1036\u001b[0m, in \u001b[0;36mrv_generic.rvs\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(cond):\n\u001b[1;32m   1031\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDomain error in arguments. The `scale` parameter must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1032\u001b[0m                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe positive for all distributions, and many \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1033\u001b[0m                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistributions have restrictions on shape parameters. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1034\u001b[0m                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see the `scipy.stats.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1035\u001b[0m                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocumentation for details.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1036\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(scale \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loc\u001b[38;5;241m*\u001b[39mones(size, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Domain error in arguments. The `scale` parameter must be positive for all distributions, and many distributions have restrictions on shape parameters. Please see the `scipy.stats.truncnorm` documentation for details."
     ]
    }
   ],
   "source": [
    "from lime import lime_tabular\n",
    "\n",
    "# Create a LIME explainer\n",
    "explainer = lime_tabular.LimeTabularExplainer(X_prueba.values, feature_names=X_prueba.columns)\n",
    "\n",
    "# Explain a prediction for a specific instance\n",
    "exp = explainer.explain_instance(X_test.iloc[0], halving_search_gb.predict, num_features=len(X_train.columns))\n",
    "\n",
    "# Visualize the explanation\n",
    "exp.show_in_notebook(show_table=True, show_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e4baf9b-0e72-4b02-9ba9-cca30210aa16",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'if_delegate_has_method' from 'sklearn.utils.metaestimators' (/Users/pameladiaz/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/sklearn/utils/metaestimators.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetaestimators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m if_delegate_has_method\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'if_delegate_has_method' from 'sklearn.utils.metaestimators' (/Users/pameladiaz/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/sklearn/utils/metaestimators.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.metaestimators import if_delegate_has_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc0f89c1-5dfc-4008-acc7-afd6274c079d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'if_delegate_has_method' from 'sklearn.utils.metaestimators' (/Users/pameladiaz/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/sklearn/utils/metaestimators.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01meli5\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming model is your trained gradient boosting model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m eli5\u001b[38;5;241m.\u001b[39mshow_weights(halving_search_gb, feature_names\u001b[38;5;241m=\u001b[39mX_prueba\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/eli5/__init__.py:13\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformatters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     format_as_html,\n\u001b[1;32m      8\u001b[0m     format_html_styles,\n\u001b[1;32m      9\u001b[0m     format_as_text,\n\u001b[1;32m     10\u001b[0m     format_as_dict,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexplain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m explain_weights, explain_prediction\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m explain_weights_sklearn, explain_prediction_sklearn\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transform_feature_names\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/eli5/sklearn/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m absolute_import\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexplain_weights\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     explain_weights_sklearn,\n\u001b[1;32m      5\u001b[0m     explain_linear_classifier_weights,\n\u001b[1;32m      6\u001b[0m     explain_linear_regressor_weights,\n\u001b[1;32m      7\u001b[0m     explain_rf_feature_importance,\n\u001b[1;32m      8\u001b[0m     explain_decision_tree,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexplain_prediction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     explain_prediction_sklearn,\n\u001b[1;32m     12\u001b[0m     explain_prediction_linear_classifier,\n\u001b[1;32m     13\u001b[0m     explain_prediction_linear_regressor,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munhashing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     InvertableHashingVectorizer,\n\u001b[1;32m     17\u001b[0m     FeatureUnhasher,\n\u001b[1;32m     18\u001b[0m     invert_hashing_and_fit,\n\u001b[1;32m     19\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/eli5/sklearn/explain_weights.py:78\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meli5\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transform_feature_names\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meli5\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_feature_importances\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     75\u001b[0m     get_feature_importances_filtered,\n\u001b[1;32m     76\u001b[0m     get_feature_importance_explanation,\n\u001b[1;32m     77\u001b[0m )\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpermutation_importance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PermutationImportance\n\u001b[1;32m     81\u001b[0m LINEAR_CAVEATS \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124mCaveats:\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124m1. Be careful with features which are not\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124m   classification result for most examples.\u001b[39m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mlstrip()\n\u001b[1;32m     93\u001b[0m HASHING_CAVEATS \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124mFeature names are restored from their hashes; this is not 100\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m precise\u001b[39m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124mbecause collisions are possible. For known collisions possible feature names\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124mthe result is positive.\u001b[39m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mlstrip()\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/eli5/sklearn/permutation_importance.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_cv\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetaestimators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m if_delegate_has_method\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_array, check_random_state\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     BaseEstimator,\n\u001b[1;32m     11\u001b[0m     MetaEstimatorMixin,\n\u001b[1;32m     12\u001b[0m     clone,\n\u001b[1;32m     13\u001b[0m     is_classifier\n\u001b[1;32m     14\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'if_delegate_has_method' from 'sklearn.utils.metaestimators' (/Users/pameladiaz/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/sklearn/utils/metaestimators.py)"
     ]
    }
   ],
   "source": [
    "import eli5\n",
    "\n",
    "# Assuming model is your trained gradient boosting model\n",
    "eli5.show_weights(halving_search_gb, feature_names=X_prueba.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67726e46-5e9b-4d59-99e1-5a64c4e6a0e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_dataset' from 'pdpbox' (/Users/pameladiaz/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/pdpbox/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpdpbox\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pdp, get_dataset\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming model is your trained gradient boosting model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m pdp_dist \u001b[38;5;241m=\u001b[39m pdp\u001b[38;5;241m.\u001b[39mpdp_isolate(halving_search_gb, X_prueba\u001b[38;5;241m.\u001b[39mcolumns, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_of_interest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_dataset' from 'pdpbox' (/Users/pameladiaz/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/pdpbox/__init__.py)"
     ]
    }
   ],
   "source": [
    "from pdpbox import pdp, get_dataset\n",
    "\n",
    "# Assuming model is your trained gradient boosting model\n",
    "pdp_dist = pdp.pdp_isolate(halving_search_gb, X_prueba, 'feature_of_interest')\n",
    "pdp.pdp_plot(pdp_dist, 'feature_of_interest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2305ffcb-fff9-4bf3-8c09-7c9e713d9858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
